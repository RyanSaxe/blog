{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Interpretable Neural Networks\n",
    "> Generalized Linear and Additive Models are well-established interpretable approaches to supervised learning. This post connects these approaches to the building blocks of Neural Networks, and demonstrates that it's possible to design Neural Networks that are just as interpretable!\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [transparency]\n",
    "- image: images/nnflow.png\n",
    "- show_description: false\n",
    "- annotations: true\n",
    "- show_tags: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#necessary packages for the notebook\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#custom matplotlib stylesheet for the blog\n",
    "plt.style.use('blog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most supervised prediction problems can be described as learning some function $f$ that minimizes the error-term ($\\epsilon$) to the equation $y = f(X) + \\epsilon$, where $X$ is the input features and $y$ is the target prediction. This blog post will explore methods to devise neural architectures to learn such functions, with the additional goal of making them as transparent as linear models. However, prior to jumping into such architectures, it's important to understand the foundation of classic transparent approaches, starting from the simplest linear models. \n",
    "\n",
    "The next section provides the necessary background to understand how the building blocks of neural networks are related to linear models. Then, we can drop the restriction of linearity with Generalized Additive Models (GAMs), and explore building transparent neural networks.\n",
    "\n",
    "# Linear Models\n",
    "\n",
    "Different algorithms are sufficient for learning different families of functions. For example, simple linear models can only learn to make predictions according to functions of the form:\n",
    "\n",
    "$$y = X^T\\beta + \\epsilon = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n + \\epsilon$$\n",
    "\n",
    "Where $\\beta_i$ represents learned coefficients with respect to $X_i$.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "Linear regression is arguably the simplest of the linear models, and comes with four assumptions:\n",
    "\n",
    "1. **Linearity**: The relationship between $X$ and the mean of $y$ is linear.\n",
    "2. **Independence**: $X_i$ and $X_j$ are linearly independent of eachother for all $i \\neq j$.\n",
    "3. **Normality**: $y$ given any $X$ comes from a normal distribution.\n",
    "4. **Homoscedasticity**: The variance of residual is the same for any value of $X$.\n",
    "\n",
    "These assumptions can be nicely described by one math equation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y & \\in \\mathcal{N}(X^T\\beta, \\sigma^2 I) \\\\\n",
    "& \\Rightarrow \\mathbb{E}[y|X] = X^T\\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, these assumptions are quite rigid for the real world. Many datasets and/or problem spaces do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can comparably perform the regression task without such rigid assumptions? The common answers to this question are:\n",
    "\n",
    "1. **Occam's Razor**: Don't add complexity without necessity.    \n",
    "2. **Little Data**: Ordinary Least Squares (OLS) is a closed form solution to linear regression{% fn 1 %}.\n",
    "3. **Interpretability**: $y$ can be explained with respect to how $X$ interacts with the $\\beta$ coefficients.\n",
    "\n",
    "Today, we are going to stick with the notion that transparency is of the utmost importance, and assume we have a significant amount of data. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, contrary to what you may have heard, these models don't need to be exactly linear as described above in order to be comparably interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "\n",
    "There are three components to any Generalized Linear Model (GLM):\n",
    "\n",
    "1. **Random Component**: The probability distribution of $y$ (typically belonging to the exponential family{% fn 2 %}).\n",
    "2. **Systematic Component**: the right side of the equation for predicting $y$ (often $X^T\\beta$).\n",
    "3. **Link Function**: A function $g$ that links the systematic component and the random component.\n",
    "\n",
    "This yields the following general equation for GLMs:\n",
    "\n",
    "$$g(\\mathbb{E}[y|X]) = X^T\\beta + \\epsilon$$\n",
    "\n",
    "Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the GLM is exactly linear regression! Hence the functions that GLMs can describe are simply a superset of the functions linear regression can describe. Furthermore, recognize that the random component loosens the constraint of normality, and that the link function alters the constraint of linearity. The relationship between $X$ and the mean of $y$ can be non-linear as long as the relationship between $X$ and the mean of $g(y)$ is linear. Lastly, the residuals are allowed to be heteroscedastic. The only restriction from linear regression that fully remains is the independence of the explanatory variables. \n",
    "\n",
    "The link function is the most important aspect to any GLM. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(-\\infty,+\\infty)$. Link functions are selected according to the given random component.\n",
    "\n",
    "For example, binary logistic regression assumes the probability distribution of $y$ is a bernoulli distribution. This means $\\mu(x)$ is between 0 and 1. We need some function $g: [0,1] \\rightarrow \\Reals$, and a logit function is sufficient for this:\n",
    "\n",
    "$$g(\\mu) = log(\\frac{\\mu}{1 - \\mu})$$\n",
    "\n",
    "Given the expected distribution of your problem, you determine a link function that will properly transform your data to the right space. And then you can fit a simple linear model: $g(y) = X^T\\beta + \\epsilon$. However, Ordinary Least Squares may no longer have a closed form solution in this case. Hence, learning $\\beta$ requires a different optimization method: Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "## Relating to Neural Networks\n",
    "\n",
    "Neural networks are algorithms built of components called layers. Layers are built of components called nodes. The first layer of a neural network is called the input layer, because each node passes an input feature. The last layer of a neural network is called the output layer, and it should represent the output you are trying to predict (this layer has one node in the classic regression case). Lastly, any layers between the input and output layers are called hidden layers.\n",
    "\n",
    "This structure of layers is ordered and connected in such a manner that each layer receives information from the previous layer, and passes it to the next. The output $X_j$ of node $j$ in layer $L_i$ gets passed to every node in the next layer in the network, $L_{i + 1}$. The image below displays a neural network with one hidden layer, and highlights the flow through a single node of that hidden layer.\n",
    "\n",
    "![](https://ryansaxe.com/images/nnflow.png) \n",
    "\n",
    "Each node contains some set of weights ($w$) and a bias ($b$). When the node receives the output ($X$) of all the nodes in the preceding layer, it performs the following computation: $X^Tw + b$. Then, before passing this output to the next layer, it gets passed to something called an activation function, which computes some (often non-linear) transform of $X^Tw + b$. The activation function is depicted as $f$ in the image above.\n",
    "\n",
    "If we substitute our definitions to $w = \\big < \\beta_1, \\beta_2, \\cdots, \\beta_n \\big >$ and $b = \\beta_0$, we are left with:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& y = f(X^T\\beta) \\\\\n",
    "& f^{-1}(y) = X^T\\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Does this look familiar? It's almost exactly the general equation of a GLM. In fact, if $f^\\inv = g$, where $g$ is a link function for a GLM, then the computation of a single node on a neural network is identical to a GLM on the output of the previous layer! \n",
    "\n",
    "This means that a neural network with zero hidden layers and a linear activation function on the output layer is exactly equivalent to linear regression. And, if we change the activation function to the inverse of the logit function (this is the sigmoid activation function), this neural network becomes exactly equivalent to logistic regression! The code below is a simple prototype of building linear and logistic regression with [Keras](https://www.keras.io), and tests it on a simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#set up parameters to generate 3 normally distributed columns\n",
    "columns = list('abc')\n",
    "beta = [1, 1, 1]\n",
    "bias = 0.0\n",
    "size = 100000\n",
    "mean = 5.0\n",
    "std = 1.0\n",
    "#create the dataset\n",
    "data = {\n",
    "    col: np.random.normal(\n",
    "        size=size, loc=mean, scale=std\n",
    "    ) for col in columns\n",
    "}\n",
    "X = pd.DataFrame(data)\n",
    "betaX = X.copy()\n",
    "for i,col in enumerate(columns):\n",
    "    betaX[col] = X[col] * beta[i]\n",
    "y_logistic = ((bias + betaX.sum(1)) < betaX.sum(1).mean()).astype(float)\n",
    "y_linear = bias + betaX.sum(1)\n",
    "#X.assign(target=y).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.996218003352118\n"
     ]
    }
   ],
   "source": [
    "cutoff = betaX.sum(1).mean()\n",
    "print(cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Regression(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=None,\n",
    "        style='linear'\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        # linear regression uses the identity as the link function\n",
    "        #    and the inverse of the identity is a linear activation\n",
    "        if style.lower() == 'linear':\n",
    "            activation = 'linear'\n",
    "        # logistic regression uses the logit link function, and the\n",
    "        #    inverse of logit is a sigmoid activation function\n",
    "        elif style.lower() == 'logistic':\n",
    "            activation = 'sigmoid'\n",
    "        # no other options are supported\n",
    "        else:\n",
    "            raise ValueError('input style only supports two options: linear or logistic')\n",
    "        # pass input directly to the output layer --- no hidden layers\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=activation)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples\n",
      "Epoch 1/20\n",
      "100000/100000 [==============================] - 4s 36us/sample - loss: 1.1346 - mean_absolute_error: 0.5068\n",
      "Epoch 2/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.6683 - mean_absolute_error: 0.4848\n",
      "Epoch 3/20\n",
      "100000/100000 [==============================] - 3s 34us/sample - loss: 0.6145 - mean_absolute_error: 0.4565\n",
      "Epoch 4/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.5672 - mean_absolute_error: 0.4293\n",
      "Epoch 5/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.5263 - mean_absolute_error: 0.4044\n",
      "Epoch 6/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.4910 - mean_absolute_error: 0.3818\n",
      "Epoch 7/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.4600 - mean_absolute_error: 0.3612\n",
      "Epoch 8/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.4331 - mean_absolute_error: 0.3428\n",
      "Epoch 9/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.4095 - mean_absolute_error: 0.3262\n",
      "Epoch 10/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.3886 - mean_absolute_error: 0.3114\n",
      "Epoch 11/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.3704 - mean_absolute_error: 0.2981\n",
      "Epoch 12/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.3541 - mean_absolute_error: 0.2861\n",
      "Epoch 13/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.3395 - mean_absolute_error: 0.2752\n",
      "Epoch 14/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.3261 - mean_absolute_error: 0.2651\n",
      "Epoch 15/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.3142 - mean_absolute_error: 0.2561\n",
      "Epoch 16/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.3036 - mean_absolute_error: 0.2480\n",
      "Epoch 17/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.2938 - mean_absolute_error: 0.2404\n",
      "Epoch 18/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.2849 - mean_absolute_error: 0.2335\n",
      "Epoch 19/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.2766 - mean_absolute_error: 0.2271\n",
      "Epoch 20/20\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 0.2690 - mean_absolute_error: 0.2211\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#logistic regression setup\n",
    "logistic_reg = Regression(style='logistic')\n",
    "logistic_reg.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "hist = logistic_reg.fit(\n",
    "    X.to_numpy(),\n",
    "    y_logistic.to_numpy(),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary crossentropy loss: 0.269 \n",
      "mean absolute error: 0.221\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print('binary crossentropy loss:',round(hist.history['loss'][-1],3),'\\nmean absolute error:',round(hist.history['mean_absolute_error'][-1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples\n",
      "Epoch 1/20\n",
      "100000/100000 [==============================] - 4s 35us/sample - loss: 11.1655 - mean_absolute_error: 2.0211\n",
      "Epoch 2/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 0.2649 - mean_absolute_error: 0.3965\n",
      "Epoch 3/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 0.0158 - mean_absolute_error: 0.0888\n",
      "Epoch 4/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 0.0013 - mean_absolute_error: 0.0272\n",
      "Epoch 5/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 2.7817e-05 - mean_absolute_error: 0.0028\n",
      "Epoch 6/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 2.2440e-11 - mean_absolute_error: 2.0603e-06\n",
      "Epoch 7/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 1.8486e-07 - mean_absolute_error: 1.5206e-04\n",
      "Epoch 8/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 1.8091e-07 - mean_absolute_error: 1.8781e-04\n",
      "Epoch 9/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 2.1534e-07 - mean_absolute_error: 1.8208e-04\n",
      "Epoch 10/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 2.3691e-07 - mean_absolute_error: 1.8331e-04\n",
      "Epoch 11/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 1.8215e-07 - mean_absolute_error: 1.9305e-04\n",
      "Epoch 12/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 2.1247e-07 - mean_absolute_error: 2.1762e-04\n",
      "Epoch 13/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 2.0063e-07 - mean_absolute_error: 1.8007e-04\n",
      "Epoch 14/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 2.2186e-07 - mean_absolute_error: 1.8071e-04\n",
      "Epoch 15/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 1.9966e-07 - mean_absolute_error: 2.3196e-04\n",
      "Epoch 16/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 2.0538e-07 - mean_absolute_error: 2.1745e-04\n",
      "Epoch 17/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 1.8733e-07 - mean_absolute_error: 2.2792e-04\n",
      "Epoch 18/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 2.2115e-07 - mean_absolute_error: 2.2728e-04\n",
      "Epoch 19/20\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 2.2025e-07 - mean_absolute_error: 2.0434e-04\n",
      "Epoch 20/20\n",
      "100000/100000 [==============================] - 3s 32us/sample - loss: 2.1161e-07 - mean_absolute_error: 1.6394e-04\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#linear regression setup\n",
    "linear_reg = Regression(style='linear')\n",
    "linear_reg.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "hist = linear_reg.fit(\n",
    "    X.to_numpy(),\n",
    "    y_linear.to_numpy(),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.0 \n",
      "mean absolute error: 0.0\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print('mean squared error:',round(hist.history['loss'][-1],3),'\\nmean absolute error:',round(hist.history['mean_absolute_error'][-1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$f(x) = \\begin{cases} 1, & \\sum_{i=1}^3 X_i < 15.0\\\\ 0, & otherwise \\end{cases}$\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath}')\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "X['target'] = betaX.sum(1)\n",
    "x_sorted = X.sort_values(by='target')\n",
    "y_before_label = bias + x_sorted['target']\n",
    "test_y = (y_before_label < x_sorted['target'].mean()).astype(float).to_numpy()\n",
    "test_X = x_sorted[columns].to_numpy(dtype=np.float32)\n",
    "\n",
    "axes[0].plot(y_before_label, linear_reg(test_X), label=\"Predicted\", lw=2)\n",
    "axes[0].plot(y_before_label,y_before_label,label=\"Ground Truth\", alpha=0.5)\n",
    "axes[0].legend()\n",
    "axes[0].set_title('$y = \\\\sum_{i=1}^3 X_i$', color=\"black\", fontsize=30)\n",
    "axes[0].set_xlabel('Sum of input features', c='white')\n",
    "axes[0].set_ylabel('Target feature', c='white')\n",
    "\n",
    "axes[1].plot(y_before_label, logistic_reg(test_X), label=\"Predicted\", lw=2)\n",
    "axes[1].plot(y_before_label,test_y,label=\"Ground Truth\", alpha=0.5)\n",
    "axes[1].plot([y_before_label.min(),y_before_label.max()], [0.5, 0.5], ls=\"--\", lw=1, label=\"cutoff\")\n",
    "axes[1].legend()\n",
    "logreg_title = r\"$f(x) = \\begin{cases} 1, & \\sum_{i=1}^3 X_i < \" + str(round(cutoff,2)) + r\"\\\\ 0, & otherwise \\end{cases}$\"\n",
    "print(logreg_title)\n",
    "axes[1].set_title(r\"$f(x) = \\begin{cases} 1, & \\sum_{i=1}^3 X_i < \"\n",
    "                  f'{round(cutoff,2)}'\n",
    "                  r\"\\\\ 0, & otherwise \\end{cases}$\", fontsize=30)\n",
    "axes[1].set_xlabel('Sum of input features', c='white')\n",
    "axes[1].set_ylabel('Target feature', c='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, It isn't always as simple as the cases of linear and logistic regression. Not all activation functions are invertible (e.g. ReLU), and hence add non-linearities in ways that are not consistent with GLMs. At the same time, the principal of the link function is to transform $y$ to the proper space, and ReLU does accomplish this under the assumption that $y$ cannot be negative.\n",
    "\n",
    "There is clearly an intimate connection between neural networks and linear models, as the computational components of neural networks are quite literally non-linear transforms on linear models. So, why are neural networks opaque and GLMs transparent? Let's look at the math for an arbitrarily defined neural network with $k$ hidden layers.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_0& = \\big < X_1, X_2, \\cdots, X_n \\big > \\\\\n",
    "L_1 & = f_1 \\Big ( \\big < L_0^Tw_{1,1} + b_{1,1}, L_0^Tw_{1,2} + b_{1,2}, \\cdots \\big > \\Big) \\\\\n",
    "& \\vdots \\\\\n",
    "L_k & = f_k \\Big( \\big < L_{k - 1}^Tw_{k,1} + b_{k,1}, L_{k - 1}^Tw_{k,2} + b_{k,2}, \\cdots \\big > \\Big) \\\\\n",
    "y & = L_k^T \\beta + \\epsilon\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $w_{i,j}$ and $b_{i,j}$ are the weights and bias of the j<sup>th</sup> node in layer $L_i$ with activation function $f_i$.\n",
    "\n",
    "While the final part of the equation is that of a linear model, observe that every element in $L_k$ is dependent on every input feature in $L_0$. This dependency is what creates the opacity of neural networks, not the non-linearity. This can be seen in the next section on Generalized Additive Models, a transparent approach with highly non-linear transforms to the input features while maintaining some notion of independence. This will shed light on how to design neural networks with control over feature dependence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#set up parameters to generate 3 normally distributed columns\n",
    "columns = ['a','b','c']\n",
    "size = 1000000\n",
    "mean = 0.0\n",
    "#set to 1.333 so that 3 stds is at around 4\n",
    "std = 1.0\n",
    "#create the dataset\n",
    "data = {\n",
    "    col: np.random.normal(\n",
    "        size=size, loc=mean, scale=std\n",
    "    ) for col in columns\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "#apply nonlinear transforms to the data\n",
    "def piecewise_crazy(x):\n",
    "    x_ = abs(x)\n",
    "    if x_ > 2:\n",
    "        return x ** 3\n",
    "    elif x_ > 1:\n",
    "        return x ** 4 * 10\n",
    "    else:\n",
    "        return x ** 5 * 100\n",
    "functions = {\n",
    "    'a': lambda x: piecewise_crazy(x),\n",
    "    'b': lambda x: x**4 + x**3 - 10 * x**2 + x,\n",
    "    'c': lambda x: 10 / ((abs(x) ** 0.5) + 1e-1),\n",
    "}\n",
    "for col in columns:\n",
    "    df[f'{col}_transform'] = df[col].apply(functions[col])\n",
    "#set up a prediction task where the target is a composition of\n",
    "#    the nonlinear transforms, but the training data is the normally\n",
    "#    distributed data prior to being transformed    \n",
    "X = df[[c for c in df.columns if not c.endswith('_transform')]]\n",
    "y = df[[c for c in df.columns if c.endswith('_transform')]].sum(1)\n",
    "#X.assign(target=y).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Additive Models\n",
    "\n",
    "\n",
    "Generalized Additive Models (GAMs) take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling:\n",
    "\n",
    "1. **Allow non-linearity**: GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth (differentiable everywhere). It also can be non-parametric.\n",
    "    \n",
    "2. **Allow dependence**: Linear models make the assumption that $X_i$ and $X_j$ are linearly independent forall $i \\neq j$. Additive models don't have this property, however we assume that which features interact are known apriori. This means that the systematic component can be an equation that contains non-linear feature interaction like $h_k(X_i,X_j, \\cdots)$.\n",
    "    \n",
    "Hence, equations for GAMs look like this:\n",
    "\n",
    "$$g(\\mu(X)) = \\beta_0 + \\beta_1 h_1(X_1) + \\beta_2 h_2(X_2, X_3) + \\cdots + \\beta_m h_m(X_n) + \\epsilon$$\n",
    "\n",
    "Technically, this makes GLMs a special case of GAMs where all functions $h$ are the identity function.\n",
    "\n",
    "## Fitting Smooth Functions\n",
    "\n",
    "## Priors and Penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "fig, axes = plt.subplots(1, len(X.columns))\n",
    "for f_idx, column in enumerate(X.columns):\n",
    "    ax = axes[f_idx]\n",
    "    start = np.quantile(df[column],0.999)\n",
    "    end = np.quantile(df[column],0.001)\n",
    "    test_x = np.expand_dims(np.linspace(start,end,100,dtype=np.float32),axis=1)\n",
    "    function = functions[column]\n",
    "    true_y = np.asarray([function(x) for x in test_x])\n",
    "    ax.plot(test_x, true_y, label=\"true_function\", color=(1.0,75.0/255.0,186.0/255.0))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pygam import LinearGAM\n",
    "from pygam import s as spline\n",
    "#fit to 20 splines of degree 3, with a low smoothing penalty \n",
    "# and no further constraints to each of the three features\n",
    "gam = LinearGAM(spline(0) + spline(1) + spline(2)).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "mae = tf.keras.metrics.MeanAbsoluteError()\n",
    "mse = tf.keras.metrics.MeanSquaredError()\n",
    "yhat = gam.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "print('mean squared error:',round(mse(yhat, y).numpy(),3),'\\nmean absolute error:',round(mae(yhat, y).numpy(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "fig, axes = plt.subplots(1, len(X.columns), figsize=(20,5))\n",
    "for i, term in enumerate(gam.terms):\n",
    "    if term.isintercept:\n",
    "        continue\n",
    "    ax = axes[i] #works because intercept is always last term\n",
    "    start = np.quantile(df[column],0.001)\n",
    "    end = np.quantile(df[column],0.999)\n",
    "    test_x = np.expand_dims(np.linspace(start,end,1000,dtype=np.float32),axis=1)\n",
    "    function = functions[columns[i]]\n",
    "    true_y = np.asarray([function(x) for x in test_x])\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    XX = XX[np.where((XX[:,i] > start) & (XX[:,i] < end))[0],:]\n",
    "    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n",
    "    ax.plot(XX[:, term.feature], pdep, label='prediction')\n",
    "    ax.plot(test_x, true_y, label=\"true_function\", alpha=0.5)\n",
    "    ax.set_title(repr(term))\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Neural Networks\n",
    "\n",
    "## Additive Neural Networks\n",
    "\n",
    "![](https://ryansaxe.com/images/nam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "#define a simple multi-layer perceptron\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        # relu helps learn more jagged functions if necessary.\n",
    "        self.l1 = layers.Dense(8, activation='relu')\n",
    "        # softplus helps smooth between the jagged areas from above\n",
    "        #     as softplus is referred to as \"SmoothRELU\"\n",
    "        self.l2 = layers.Dense(8, activation='softplus')\n",
    "        self.l3 = layers.Dense(8, activation='softplus')\n",
    "        self.l4 = layers.Dense(8, activation='softplus')\n",
    "        self.output_layer = layers.Dense(1)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "#define a Neural Additive Model for n features\n",
    "class NAM(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        name=None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        self.n_features = n_features\n",
    "        # initialize MLP for each input feature\n",
    "        self.components = [MLP() for i in range(n_features)]\n",
    "        # create final layer for a linear combination of learned components\n",
    "        self.linear_combination = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x, training=None):\n",
    "        #split up by individual features\n",
    "        individual_features = tf.split(x, self.n_features, axis=1)\n",
    "        components = []\n",
    "        #apply the proper MLP to each individual feature\n",
    "        for f_idx,individual_feature in enumerate(individual_features):\n",
    "            component = self.components[f_idx](individual_feature)\n",
    "            components.append(component)\n",
    "        #concatenate learned components and return linear combination of them\n",
    "        components = tf.concat(components, axis=1)\n",
    "        return self.linear_combination(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hide_output\n",
    "model = NAM(n_features=3)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "hist = model.fit(\n",
    "    X.to_numpy(),\n",
    "    y.to_numpy(),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "print('mean squared error:',round(hist.history['loss'][-1],3),'\\nmean absolute error:',round(hist.history['mean_absolute_error'][-1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "fig, axes = plt.subplots(1, len(X.columns), figsize=(20,5))\n",
    "for f_idx, column in enumerate(X.columns):\n",
    "    ax = axes[f_idx]\n",
    "    multiplier = model.linear_combination.weights[0].numpy()[f_idx]\n",
    "    start = np.quantile(df[column],0.999)\n",
    "    end = np.quantile(df[column],0.001)\n",
    "    test_x = np.expand_dims(np.linspace(start,end,100,dtype=np.float32),axis=1)\n",
    "    function = functions[column]\n",
    "    true_y = np.asarray([function(x) for x in test_x])\n",
    "    pred_y = model.components[f_idx](test_x).numpy() * multiplier\n",
    "    ax.plot(test_x, pred_y, label='predicted_function')\n",
    "    ax.plot(test_x, true_y, label='true_function', alpha=0.5)\n",
    "    relu_outs = model.components[f_idx].l1(test_x)\n",
    "    vlines = 0\n",
    "    for i in range(8):\n",
    "        relu_out = relu_outs[:, i]\n",
    "        nonzeros = np.where(relu_out)[0]\n",
    "        if len(nonzeros) == 0:\n",
    "            continue\n",
    "        start = nonzeros.min()\n",
    "        end = nonzeros.max()\n",
    "        if start == 0 and end == 99:\n",
    "            continue\n",
    "        elif start == 0:\n",
    "            val = test_x[end + 1][0]\n",
    "        else:\n",
    "            val = test_x[start - 1][0]\n",
    "        ax.axvline(x=val, alpha=0.6, color=(186.0/255.0,1.0,75.0/255.0), ls=\"--\", lw=1)\n",
    "        vlines += 1\n",
    "    title = f'function for column {column}'\n",
    "    ax.set_title(title)\n",
    "    relu_breaks = plt.Line2D((0,1),(0,0), color=(186.0/255.0,1.0,75.0/255.0), ls=\"--\", lw=1)\n",
    "    display = (0,1,2)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    #Create legend from custom artist/label lists\n",
    "    ax.legend([handle for i,handle in enumerate(handles) if i in display]+[relu_breaks],\n",
    "              [label for i,label in enumerate(labels) if i in display]+['RELU Breakpoints'])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, these results look good, but not perfect. While the Neural Additive Model was capable of learning the general shapes of the functions, it looks like it's off on the intercepts for all of them. This should not discount the validity of the model. The corresponding math demonstrates perfectly fitting intercepts of an additive model cannot be guaranteed.\n",
    "\n",
    "Let $h_i(X_i) = \\alpha_i + f(X_i)$ where $f(X_i)$ represents all of the aspects of $h(X_i)$ that dependent on $X_i$, and $\\alpha_i$ represents the intercept.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y & = \\beta_0 + \\sum_{i=1}^n \\beta_i h_i(X_i) \\\\\n",
    "& = \\beta_0 + \\sum_{i=1}^n \\beta_i (\\alpha_i + f_i(X_i)) \\\\\n",
    "& = \\beta_0 + \\sum_{i=1}^n \\beta_i \\alpha_i + \\sum_{i=1}^n \\beta_i f_i(X_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The only way to tease apart these intercepts is via $\\beta$. Imagine the proper fit of this equation, for every $i$, had $\\beta_i = 1$ and $\\alpha_i = 2$. In this case, if half of the learned $\\alpha_i$s are zero, and the other half are four, that would yield the exact same result for $\\sum_{i=1}^n \\beta_i \\alpha_i$ as the proper fit. Hence, by way of contradictory example, it is impossible to guarantee learning correct intercepts for any additive model.\n",
    "\n",
    "However, the \"goodness of fit\" of this model to the true functions can be shown by comparing the partial derivatives. This is because, when taking a derivative with respect to $X$, any aspects of the equation that don't depend on $X$ (e.g. the intercepts) become zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#y &= \\beta_0 + \\sum_{i=1}^n \\beta_i \\alpha_i + \\sum_{i=1}^n \\beta_i f_i(X_i)\n",
    "\n",
    "def dxdy(x, y):\n",
    "    return (x[1:] - x[:-1])/(y[1:] - y[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "fig, axes = plt.subplots(1, len(X.columns), figsize=(20,5))\n",
    "#fig2, axes2 = plt.subplots(1, len(X.columns), figsize=(20,5))\n",
    "for f_idx, column in enumerate(X.columns):\n",
    "    ax = axes[f_idx]\n",
    "    multiplier = model.linear_combination.weights[0].numpy()[f_idx]\n",
    "    start = np.quantile(df[column],0.999)\n",
    "    end = np.quantile(df[column],0.001)\n",
    "    test_x = np.expand_dims(np.linspace(start,end,100,dtype=np.float32),axis=1)\n",
    "    function = functions[column]\n",
    "    true_y = np.asarray([function(x) for x in test_x])\n",
    "    pred_y = model.components[f_idx](test_x).numpy() * multiplier\n",
    "    relu_outs = model.components[f_idx].l1(test_x)\n",
    "    vlines = 0\n",
    "    for i in range(8):\n",
    "        relu_out = relu_outs[:, i]\n",
    "        nonzeros = np.where(relu_out)[0]\n",
    "        if len(nonzeros) == 0:\n",
    "            continue\n",
    "        start = nonzeros.min()\n",
    "        end = nonzeros.max()\n",
    "        if start == 0 and end == 99:\n",
    "            continue\n",
    "        elif start == 0:\n",
    "            val = test_x[end + 1][0]\n",
    "        else:\n",
    "            val = test_x[start - 1][0]\n",
    "        ax.axvline(x=val, alpha=0.5, color=(186.0/255.0,1.0,75.0/255.0), ls=\"--\", lw=1)\n",
    "        vlines += 1\n",
    "    ax.plot(test_x[1:], dxdy(pred_y,test_x), label='predicted_function')\n",
    "    ax.plot(test_x[1:], dxdy(true_y,test_x), label='true_function',alpha=0.5)\n",
    "    title = f'derivative for column {column} with {vlines} learned breakpoints'\n",
    "    ax.set_title(title)\n",
    "    relu_breaks = plt.Line2D((0,1),(0,0), color=(186.0/255.0,1.0,75.0/255.0), ls=\"--\", lw=1)\n",
    "    display = (0,1,2)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    #Create legend from custom artist/label lists\n",
    "    ax.legend([handle for i,handle in enumerate(handles) if i in display]+[relu_breaks],\n",
    "              [label for i,label in enumerate(labels) if i in display]+['RELU Breakpoints'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Additive Models\n",
    "\n",
    "## Generalizations\n",
    "\n",
    "This is the fun part. This is where all your creativity for problem solving given domain knowledge yields an extremely flexible, powerful, and interpretable model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ \"The closed form solution for OLS is $\\beta = (X^TX)^\\inv X^Ty$. This requires $X^TX$ to be invertible, which is the case when the elements in $X$ are linearly independent. This is satisfied by our assumption of independence. Without this assumption, there is no closed form solution, and $\\beta$ can be approximated by the maximum likelihood estimation function: $min_\\beta(y - \\beta X)^T(y - \\beta X)$.\" | fndetail: 1 }}\n",
    "\n",
    "{{ 'The exponential family is a particular family of probability distributions such that their probability density function (PDF) can be writted as: $P(x | \\theta) = f(x) g(\\theta) exp \\Big( \\eta(\\theta) \\centerdot T(x) \\Big)$, where $f$, $g$, $\\eta$, and $T$ are known functions and $\\theta \\in \\Reals$ is the only parameter to the PDF.' | fndetail: 2 }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
