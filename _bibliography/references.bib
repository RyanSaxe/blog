@article{pinkus_1999,
    title={Approximation theory of the MLP model in neural networks}, 
    volume={8}, 
    DOI={10.1017/S0962492900002919}, 
    journal={Acta Numerica}, 
    publisher={Cambridge University Press}, 
    author={Pinkus, Allan}, 
    year={1999}, 
    pages={143–195}
}

@misc{nam_2020,
      title={Neural Additive Models: Interpretable Machine Learning with Neural Nets}, 
      author={Rishabh Agarwal and Nicholas Frosst and Xuezhou Zhang and Rich Caruana and Geoffrey E. Hinton},
      year={2020},
      eprint={2004.13912},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{univ_approx_thm,
    author={Csáji Balázs},
    title={Approximation with Artificial Neural Networks},
    year={2001},
    DOI={10.1.1.101.2647}
}

@article{univ_approx_orig,
    title = {Multilayer feedforward networks are universal approximators},
    journal = {Neural Networks},
    volume = {2},
    number = {5},
    pages = {359 - 366},
    year = {1989},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
    url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
    author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
    keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
    abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{GAM,
    author = {Hastie, Trevor and Tibshirani, Robert},
    doi = {10.1214/ss/1177013604},
    fjournal = {Statistical Science},
    journal = {Statist. Sci.},
    month = {08},
    number = {3},
    pages = {297--310},
    publisher = {The Institute of Mathematical Statistics},
    title = {Generalized Additive Models},
    url = {https://doi.org/10.1214/ss/1177013604},
    volume = {1},
    year = {1986}
}

@article{backfitting_two,
    author = { Breiman, Leo and Friedman, Jerome H.},
    title = {Estimating Optimal Transformations for Multiple Regression and Correlation},
    journal = {Journal of the American Statistical Association},
    volume = {80},
    number = {391},
    pages = {580-598},
    year  = {1985},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.1985.10478157},
}


@article{backfitting_orig,
    author = { Jerome H.   Friedman  and  Werner   Stuetzle },
    title = {Projection Pursuit Regression},
    journal = {Journal of the American Statistical Association},
    volume = {76},
    number = {376},
    pages = {817-823},
    year  = {1981},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.1981.10477729},

    URL = { 
            https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477729
        
    },
    eprint = { 
            https://www.tandfonline.com/doi/pdf/10.1080/01621459.1981.10477729
        
    }

}

@book{bspline,
author = {Prautzsch, Hartmut and Boehm, Wolfgang and Paluszny, Marco},
year = {2002},
month = {01},
pages = {},
title = {Bézier and B-Spline Techniques},
isbn = {978-3-642-07842-2},
doi = {10.1007/978-3-662-04919-8}
}

@article{bspline_gauss,
author = {Wang, Yu-Ping and Lee, S. L.},
year = {1998},
month = {11},
pages = {1040 - 1055},
title = {Scale-space derived from B-splines},
volume = {20},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
doi = {10.1109/34.722612}
}

@inproceedings{GANN,
    author = {Potts, William J. E.},
    title = {Generalized Additive Neural Networks},
    year = {1999},
    isbn = {1581131437},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/312129.312228},
    doi = {10.1145/312129.312228},
    booktitle = {Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {194–200},
    numpages = {7},
    keywords = {predictive modeling, partial residuals, additive models},
    location = {San Diego, California, USA},
    series = {KDD '99}
}

@misc{piecewise_relu,
      title={Understanding Deep Neural Networks with Rectified Linear Units}, 
      author={Raman Arora and Amitabh Basu and Poorya Mianjy and Anirbit Mukherjee},
      year={2018},
      eprint={1611.01491},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{GLM,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2344614},
 abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
 author = {J. A. Nelder and R. W. M. Wedderburn},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {3},
 pages = {370--384},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Generalized Linear Models},
 volume = {135},
 year = {1972}
}

