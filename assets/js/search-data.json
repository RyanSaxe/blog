{
  
    
        "post0": {
            "title": "Designing Interpretable Neural Networks",
            "content": "Most supervised prediction problems can be described as learning some function $f$ that minimizes the error-term ($ epsilon$) to the equation $y = f(X) + epsilon$, where $X$ is the input features and $y$ is the target prediction. This blog post will explore methods to devise neural architectures to learn such functions, with the additional goal of making them as transparent as linear models. However, prior to jumping into such architectures, it&#39;s important to understand the foundation of classic transparent approaches, starting from the simplest linear models. . The next section provides the necessary background to understand how the building blocks of neural networks are related to linear models. Then, we can drop the restriction of linearity with Generalized Additive Models (GAMs), and explore building transparent neural networks. . Linear Models . Different algorithms are sufficient for learning different families of functions. For example, simple linear models can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_n X_n + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$. . Linear Regression . Linear regression is arguably the simplest of the linear models, and comes with four assumptions: . Linearity: The relationship between $X$ and the mean of $Y$ is linear. | Independence: $X_i$ and $X_j$ are independent of eachother for all $i neq j$. | Normality: $y$ given any $X$ comes from a normal distribution. | Homoscedasticity: The variance of residual is the same for any value of $X$. | These assumptions can be nicely described by one math equation: . $$ begin{align} y &amp; in mathcal{N}(X^T beta, sigma^2 I) &amp; Rightarrow mathbb{E}[y|X] = X^T beta_{test} end{align} $$However, these assumptions are quite rigid for the real world. Many datasets and/or problem spaces do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can comparably perform the regression task without such rigid assumptions? The common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. | Little Data: Ordinary Least Squares (OLS) is an estimation method for linear regression with a closed form solution1. | Interpretability: The prediction $y$ can always be explained with respect to the input features $X$ connected to the learned coefficients $ beta$. | Today, we are going to stick with the notion that transparency is of the utmost importance, and assume we have a significant amount of data. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, contrary to what you may have heard, these models don&#39;t need to be exactly linear as described above in order to be comparably interpretable. . Generalized Linear Models . There are three components to any Generalized Linear Model (GLM): . Random Component: The probability distribution of $y$ (typically belonging to the exponential family2). | Systematic Component: the system of explanatory variables for predicting the random component (often $X^T beta$). | Link Function: A function $g$ that links systematic component and the random component. | This yields the following general equation for GLMs: . $$g( mathbb{E}[y|X]) = X^T beta + epsilon$$ . Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the GLM is exactly linear regression! Hence the functions that GLMs can describe are simply a superset of the functions linear regression can describe. Furthermore, recognize that the random component loosens the constraint of normality, and that the link function alters the constraint of linearity. The relationship between $X$ and the mean of $y$ can be non-linear as long as the relationship between $X$ and the mean of $g(y)$ is linear. Lastly, the residuals are allowed to be heteroscedastic. The only restriction from linear regression that fully remains is the independence of the explanatory variables. . The Link Function . Non-normal random components necessitate a link function. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(- infty,+ infty)$. . However, not all of these distributions range from $- infty$ to $+ infty$, which is necessary for solving regression problems in the linear form $y = X^T beta$. In order for us to maintain a linear model, we transform the mean of the distribution ( $ mu(x) = mathbb{E}[y|X]$ ) using a specified function $g$. $g$ is called the link function. . $$g( mu(x)) = X^T beta + epsilon$$ . Observe that this also loosens the constraints of linearity. Recall that $ mu(x)$ is just our expectation for $y$ given $X$. So, if we substitute back $y$, and take the inverse to get rid of $g$ in relationship to $y$, we get: . $$y = g^{-1}(X^T beta) + epsilon$$ . Hence the relationship between $y$ and $X$ is no longer required to be exactly linear. However, the relationship between the resulting $g(y)$ transformed by the link function and $X$ must be linear. . Note that this clarification implies that L $g$ is the identity function $g( mu) = mu$, then the GLM is just normal linear regression. . Maximum Likelihood Estimation . Note: OLS = MLE under normality constraint . Example: Logistic Regression . Logistic regression is actually a GLM! Let&#39;s say we&#39;re trying to predict some binary variable, which means that $y$ will be either 0 or 1. This can be described by a bernoulli distribution. In order to squash the regression results to be between 0 and 1, we use the logit link function $g( mu) = log( frac{ mu}{1 - mu})$. . And GLMs are as easy as that! Given the expected distribution of your problem, you determine a link function that will properly transform your data to the right space. And then you can fit a simple linear model: $y = X^T beta$. . Relating to Neural Networks . Note: MLE -&gt; crossentropy . . Link Function -&gt; Activation Function . Maximum Likelihood Estimation -&gt; Loss Function . Code Example . class Regression(tf.keras.Model): def __init__( self, name=None, style=&#39;linear&#39; ): super().__init__(name=name) # linear regression uses the identity as the link function # and the inverse of the identity is a linear activation if style.lower() == &#39;linear&#39;: activation = &#39;linear&#39; # logistic regression uses the logit link function, and the # inverse of logit is a sigmoid activation function elif style.lower() == &#39;logistic&#39;: activation = &#39;sigmoid&#39; # no other options are supported else: raise ValueError(&#39;input style only supports two options: linear or logistic&#39;) # pass input directly to the output layer no hidden layers self.output_layer = tf.keras.layers.Dense(1, activation=activation) def call(self, x, training=None): return self.output_layer(x) . #logistic regression setup logistic_reg = Regression(style=&#39;logistic&#39;) logistic_reg.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = logistic_reg.fit( X.to_numpy(), y_logistic.to_numpy(), epochs=20, batch_size=32, ) . binary crossentropy loss: 0.26264412118434904 mean absolute error: 0.21618827 . #linear regression setup linear_reg = Regression(style=&#39;linear&#39;) linear_reg.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = linear_reg.fit( X.to_numpy(), y_linear.to_numpy(), epochs=20, batch_size=32, ) . mean squared error: 1.9702294863463975e-07 mean absolute error: 0.00019862552 . Generalized Additive Models . Generalized Additive Models (GAMs) take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling: . Full elimination of the linearity restriction. . GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth (differentiable everywhere). It also can be non-parametric. . | A change to the independence criterion. . Linear models make the assumption that $X_i$ and $X_j$ are independent forall $i neq j$. Additive models don&#39;t have this property, however we assume that which features interact are known apriori. This means that the systematic component can be an equation that contains non-linear feature interaction like $h_k(X_i,X_j, cdots)$. . | Hence, equations for GAMs look like this: . $$g( mu(X)) = beta_0 + beta_1 h_1(X_1) + beta_2 h_2(X_2, X_3) + cdots + beta_m h_m(X_n) + epsilon$$ . Technically, this makes GLMs a special case of GAMs where all functions $h$ . Fitting Smooth Functions . from pygam import LinearGAM from pygam import s as spline #fit to 20 splines of degree 3, with a low smoothing penalty # and no further constraints to each of the three features gam = LinearGAM(spline(0) + spline(1) + spline(2)).fit(X, y) . mean squared error: 411.93893 mean absolute error: 13.017934 . Additive Neural Networks . Example Code . from tensorflow.keras import layers, regularizers #define a simple multi-layer perceptron class MLP(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) # relu helps learn more jagged functions if necessary. self.l1 = layers.Dense(8, activation=&#39;relu&#39;) # softplus helps smooth between the jagged areas from above # as softplus is referred to as &quot;SmoothRELU&quot; self.l2 = layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Neural Additive Model for n features class NAM(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features # initialize MLP for each input feature self.components = [MLP() for i in range(n_features)] # create final layer for a linear combination of learned components self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): #split up by individual features individual_features = tf.split(x, self.n_features, axis=1) components = [] #apply the proper MLP to each individual feature for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) #concatenate learned components and return linear combination of them components = tf.concat(components, axis=1) return self.linear_combination(components) . model = NAM(n_features=3) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=30, batch_size=32, ) . mean squared error: 33.171770990338324 mean absolute error: 2.0058768 . At first glance, these results look good, but not perfect. While the Neural Additive Model was capable of learning the general shapes of the functions, it looks like it&#39;s off on the intercepts for all of them. This should not discount the validity of the model. The corresponding math demonstrates perfectly fitting intercepts of an additive model cannot be guaranteed. . Let $h_i(X_i) = alpha_i + f(X_i)$ where $f(X_i)$ represents all of the aspects of $h(X_i)$ that dependent on $X_i$, and $ alpha_i$ represents the intercept. . $$ begin{align} y &amp;= beta_0 + sum_{i=1}^n beta_i h_i(X_i) y &amp;= beta_0 + sum_{i=1}^n beta_i ( alpha_i + f_i(X_i)) y &amp;= beta_0 + sum_{i=1}^n beta_i alpha_i + sum_{i=1}^n beta_i f_i(X_i) end{align} $$The only way to tease apart these intercepts is via $ beta$. Imagine the proper fit of this equation, for every $i$, had $ beta_i = 1$ and $ alpha_i = 2$. In this case, if half of the learned $ alpha_i$s are zero, and the other half are four, that would yield the exact same result for $ sum_{i=1}^n beta_i alpha_i$ as the proper fit. Hence, by way of contradictory example, it is impossible to guarantee learning correct intercepts for any additive model. . However, the &quot;goodness of fit&quot; of this model to the true functions can be shown by comparing the partial derivatives. This is because, when taking a derivative with respect to $X$, any aspects of the equation that don&#39;t depend on $X$ (e.g. the intercepts) become zero. . Interpretable Neural Networks . This is the fun part. This is where all your creativity for problem solving given domain knowledge yields an extremely flexible, powerful, and interpretable model! . Priors . Penalties . 1. The closed form solution for OLS is $ beta = (X^TX)^1 X^Ty$. This requires $X^TX$ to be invertible, which is the case when the elements in $X$ are linearly independent, which is satisfied by our assumption of independence. Without this, there is no closed form solution, and $ beta$ can be approximated by the maximum likelihood estimation function: $min_ beta(y - beta X)^T(y - beta X)$.↩ . 2. The exponential family is a particular family of probability distributions containing an exponential in their probability density function. For further light reading on the topic, please refer to the Wiki.↩ .",
            "url": "https://ryansaxe.com/transparency/2020/12/01/NAM.html",
            "relUrl": "/transparency/2020/12/01/NAM.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  

  

  

  
  

  

  
  

  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}