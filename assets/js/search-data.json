{
  
    
        "post0": {
            "title": "Designing Interpretable Neural Networks",
            "content": "Most supervised prediction problems can be described as learning some function $f$ that minimizes the error-term ($ epsilon$) to the equation $y = f(X) + epsilon$, where $X$ is the input features and $y$ is the target prediction. This blog post will explore methods to devise neural architectures to learn such functions, with the additional goal of making them as transparent as linear models. However, prior to jumping into such architectures, it&#39;s important to understand the foundation of classic transparent approaches, starting from the simplest linear models. . The next section provides the necessary background to understand how the building blocks of neural networks are related to linear models. Then, we can drop the restriction of linearity with Generalized Additive Models (GAMs), and explore building transparent neural networks. . Linear Models . Different algorithms are sufficient for learning different families of functions. For example, simple linear models can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_n X_n + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$. . Linear Regression . Linear regression is arguably the simplest of the linear models, and comes with four assumptions: . Linearity: The relationship between $X$ and the mean of $Y$ is linear. | Independence: $X_i$ and $X_j$ are independent of eachother for all $i neq j$. | Normality: $y$ given any $X$ comes from a normal distribution. | Homoscedasticity: The variance of residual is the same for any value of $X$. | These assumptions can be nicely described by one math equation: . $$ begin{aligned} y &amp; in mathcal{N}(X^T beta, sigma^2 I) &amp; Rightarrow mathbb{E}[y|X] = X^T beta end{aligned} $$However, these assumptions are quite rigid for the real world. Many datasets and/or problem spaces do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can comparably perform the regression task without such rigid assumptions? The common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. | Little Data: Ordinary Least Squares (OLS) is a closed form solution to linear regression1. | Interpretability: $y$ can be explained with respect to how $X$ interacts with the $ beta$ coefficients. | Today, we are going to stick with the notion that transparency is of the utmost importance, and assume we have a significant amount of data. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, contrary to what you may have heard, these models don&#39;t need to be exactly linear as described above in order to be comparably interpretable. . Generalized Linear Models . There are three components to any Generalized Linear Model (GLM): . Random Component: The probability distribution of $y$ (typically belonging to the exponential family2). | Systematic Component: the right side of the equation for predicting $y$ (often $X^T beta$). | Link Function: A function $g$ that links the systematic component and the random component. | This yields the following general equation for GLMs: . $$g( mathbb{E}[y|X]) = X^T beta + epsilon$$ . Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the GLM is exactly linear regression! Hence the functions that GLMs can describe are simply a superset of the functions linear regression can describe. Furthermore, recognize that the random component loosens the constraint of normality, and that the link function alters the constraint of linearity. The relationship between $X$ and the mean of $y$ can be non-linear as long as the relationship between $X$ and the mean of $g(y)$ is linear. Lastly, the residuals are allowed to be heteroscedastic. The only restriction from linear regression that fully remains is the independence of the explanatory variables. . The link function is the most important aspect to any GLM. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(- infty,+ infty)$. Link functions are selected according to the given random component. . For example, binary logistic regression assumes the probability distribution of $y$ is a bernoulli distribution. This means $ mu(x)$ is between 0 and 1. We need some function $g: [0,1] rightarrow Reals$, and a logit function is sufficient for this: . $$g( mu) = log( frac{ mu}{1 - mu})$$ . Given the expected distribution of your problem, you determine a link function that will properly transform your data to the right space. And then you can fit a simple linear model: $g(y) = X^T beta + epsilon$. However, Ordinary Least Squares may no longer have a closed form solution in this case. Hence, learning $ beta$ requires a different optimization method: Maximum Likelihood Estimation (MLE). . Relating to Neural Networks . Neural networks are algorithms built of components called layers. Layers are built of components called nodes. The first layer of a neural network is called the input layer, because each node passes an input feature. The last layer of a neural network is called the output layer, and it should represent the output you are trying to predict (this layer has one node in the classic regression case). Lastly, any layers between the input and output layers are called hidden layers. . This structure of layers is ordered and connected in such a manner that each layer receives information from the previous layer, and passes it to the next. The output $X_j$ of node $j$ in layer $L_i$ gets passed to every node in the next layer in the network, $L_{i + 1}$. The image below displays a neural network with one hidden layer, and highlights the flow through a single node of that hidden layer. . . Each node contains some set of weights ($w$) and a bias ($b$). When the node receives the output ($X$) of all the nodes in the preceding layer, it performs the following computation: $X^Tw + b$. Then, before passing this output to the next layer, it gets passed to something called an activation function, which computes some (often non-linear) transform of $X^Tw + b$. The activation function is depicted as $f$ in the image above. . If we substitute our definitions to $w = big &lt; beta_1, beta_2, cdots, beta_n big &gt;$ and $b = beta_0$, we are left with: . $$ begin{aligned} &amp; y = f(X^T beta) &amp; f^{-1}(y) = X^T beta end{aligned} $$Does this look familiar? It&#39;s almost exactly the general equation of a GLM. In fact, if $f^ inv = g$, where $g$ is a link function for a GLM, then the computation of a single node on a neural network is identical to a GLM on the output of the previous layer! . This means that a neural network with zero hidden layers and a linear activation function on the output layer is exactly equivalent to linear regression. And, if we change the activation function to the inverse of the logit function (this is the sigmoid activation function), this neural network becomes exactly equivalent to logistic regression! The code below is a simple prototype of building linear and logistic regression with Keras, and tests it on a simulated dataset. . class Regression(tf.keras.Model): def __init__( self, name=None, style=&#39;linear&#39; ): super().__init__(name=name) # linear regression uses the identity as the link function # and the inverse of the identity is a linear activation if style.lower() == &#39;linear&#39;: activation = &#39;linear&#39; # logistic regression uses the logit link function, and the # inverse of logit is a sigmoid activation function elif style.lower() == &#39;logistic&#39;: activation = &#39;sigmoid&#39; # no other options are supported else: raise ValueError(&#39;input style only supports two options: linear or logistic&#39;) # pass input directly to the output layer no hidden layers self.output_layer = tf.keras.layers.Dense(1, activation=activation) def call(self, x, training=None): return self.output_layer(x) . However, It isn&#39;t always as simple as the cases of linear and logistic regression. Not all activation functions are invertible (e.g. ReLU), and hence add non-linearities in ways that are not consistent with GLMs. At the same time, the principal of the link function is to transform $y$ to the proper space, and ReLU does accomplish this under the assumption that $y$ cannot be negative. . There is clearly an intimate connection between neural networks and linear models, as the computational components of neural networks are quite literally non-linear transforms on linear models. So, why are neural networks opaque and GLMs transparent? Let&#39;s look at the math for an arbitrarily defined neural network with $k$ hidden layers. We will denote : . $$ begin{aligned} L_0&amp; = big &lt; X_1, X_2, cdots, X_n big &gt; L_1 &amp; = f_1 Big ( big &lt; L_0^Tw_{1,1} + b_{1,1}, L_0^Tw_{1,2} + b_{1,2}, cdots big &gt; Big) &amp; vdots L_k &amp; = f_k Big( big &lt; L_{k - 1}^Tw_{k,1} + b_{k,1}, L_{k - 1}^Tw_{k,2} + b_{k,2}, cdots big &gt; Big) y &amp; = L_k^T beta + epsilon end{aligned} $$Where $w_{i,j}$ and $b_{i,j}$ are the weights and bias of the jth node i layer $L_i$ with activation function $f_i$. . While the final part of the equation is that of a linear model, observe that every element in $L_k$ is dependent on every input feature in $L_0$. This dependency is what creates the opacity of neural networks, not the non-linearity. This can be seen in the next section on Generalized Additive Models, a transparent approach with highly non-linear transforms to the input features while maintaining some notion of independence. This will shed light on how to design neural networks with control over feature dependence! . Generalized Additive Models . Generalized Additive Models (GAMs) take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling: . Allow non-linearity: GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth (differentiable everywhere). It also can be non-parametric. . | Allow dependence: Linear models make the assumption that $X_i$ and $X_j$ are independent forall $i neq j$. Additive models don&#39;t have this property, however we assume that which features interact are known apriori. This means that the systematic component can be an equation that contains non-linear feature interaction like $h_k(X_i,X_j, cdots)$. . | Hence, equations for GAMs look like this: . $$g( mu(X)) = beta_0 + beta_1 h_1(X_1) + beta_2 h_2(X_2, X_3) + cdots + beta_m h_m(X_n) + epsilon$$ . Technically, this makes GLMs a special case of GAMs where all functions $h$ are the identity function. . Fitting Smooth Functions . Priors and Penalties . from pygam import LinearGAM from pygam import s as spline #fit to 20 splines of degree 3, with a low smoothing penalty # and no further constraints to each of the three features gam = LinearGAM(spline(0) + spline(1) + spline(2)).fit(X, y) . mean squared error: 411.93893 mean absolute error: 13.017934 . Interpretable Neural Networks . Additive Neural Networks . from tensorflow.keras import layers, regularizers #define a simple multi-layer perceptron class MLP(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) # relu helps learn more jagged functions if necessary. self.l1 = layers.Dense(8, activation=&#39;relu&#39;) # softplus helps smooth between the jagged areas from above # as softplus is referred to as &quot;SmoothRELU&quot; self.l2 = layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Neural Additive Model for n features class NAM(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features # initialize MLP for each input feature self.components = [MLP() for i in range(n_features)] # create final layer for a linear combination of learned components self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): #split up by individual features individual_features = tf.split(x, self.n_features, axis=1) components = [] #apply the proper MLP to each individual feature for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) #concatenate learned components and return linear combination of them components = tf.concat(components, axis=1) return self.linear_combination(components) . model = NAM(n_features=3) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=30, batch_size=32, ) . mean squared error: 33.171770990338324 mean absolute error: 2.0058768 . At first glance, these results look good, but not perfect. While the Neural Additive Model was capable of learning the general shapes of the functions, it looks like it&#39;s off on the intercepts for all of them. This should not discount the validity of the model. The corresponding math demonstrates perfectly fitting intercepts of an additive model cannot be guaranteed. . Let $h_i(X_i) = alpha_i + f(X_i)$ where $f(X_i)$ represents all of the aspects of $h(X_i)$ that dependent on $X_i$, and $ alpha_i$ represents the intercept. . $$ begin{aligned} y &amp; = beta_0 + sum_{i=1}^n beta_i h_i(X_i) &amp; = beta_0 + sum_{i=1}^n beta_i ( alpha_i + f_i(X_i)) &amp; = beta_0 + sum_{i=1}^n beta_i alpha_i + sum_{i=1}^n beta_i f_i(X_i) end{aligned} $$The only way to tease apart these intercepts is via $ beta$. Imagine the proper fit of this equation, for every $i$, had $ beta_i = 1$ and $ alpha_i = 2$. In this case, if half of the learned $ alpha_i$s are zero, and the other half are four, that would yield the exact same result for $ sum_{i=1}^n beta_i alpha_i$ as the proper fit. Hence, by way of contradictory example, it is impossible to guarantee learning correct intercepts for any additive model. . However, the &quot;goodness of fit&quot; of this model to the true functions can be shown by comparing the partial derivatives. This is because, when taking a derivative with respect to $X$, any aspects of the equation that don&#39;t depend on $X$ (e.g. the intercepts) become zero. . Neural Additive Models . Generalizations . This is the fun part. This is where all your creativity for problem solving given domain knowledge yields an extremely flexible, powerful, and interpretable model! . 1. The closed form solution for OLS is $ beta = (X^TX)^ inv X^Ty$. This requires $X^TX$ to be invertible, which is the case when the elements in $X$ are linearly independent. This is satisfied by our assumption of independence. Without this assumption, there is no closed form solution, and $ beta$ can be approximated by the maximum likelihood estimation function: $min_ beta(y - beta X)^T(y - beta X)$.↩ . 2. The exponential family is a particular family of probability distributions such that their probability density function (PDF) can be writted as: $P(x | theta) = f(x) g( theta) exp Big( eta( theta) centerdot T(x) Big)$, where $f$, $g$, $ eta$, and $T$ are known functions and $ theta in Reals$ is the only parameter to the PDF.↩ .",
            "url": "https://ryansaxe.com/transparency/2020/12/01/NAM.html",
            "relUrl": "/transparency/2020/12/01/NAM.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  

  

  

  
  

  

  
  

  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}