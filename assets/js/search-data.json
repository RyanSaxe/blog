{
  
    
        "post0": {
            "title": "Designing Interpretable Neural Networks",
            "content": "Any supervised prediction problem can be described as learning some function $f$ that minimizes the error to the equation $f(x) = y$, where $x$ is the input features and $y$ is the target prediction. This blog post will explore methods to devise neural architectures to learn such functions, with the additional goal of making them as transparent as linear models. . Prior to delving straight into the neural networks, we should work our way up the hierarchy of linear models. Starting from basic linear models, to generalizing them with the aptly named General Linear Model (GLM), and eventually removing linearity with Generalized Additive Models (GLMs) and beyond! . Linear Regression . Different algorithms are sufficient for learning different families of functions. For example, simple linear models like linear regression can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_N X_N + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$, and $ epsilon$ is the error-term (often referred to as the residual). Furthermore, linear regression comes with some additional assumptions: . Linearity: The relationship between $X$ and the mean of $Y$ . The namesake of linear models: linearity. This is a rigid assumption. Linear regression cannot properly solve a problem where this does not hold. In fact, a lack of this relationship will often be reflected in the error terms in a way that breaks the other assumptions. . | Independence: Observations are independent of each other. . Linear regression assumes that there are no relationships between the features. For example, a model that has a feature for price and a feature for discount would break this assumption. As discount changes, the price changes accordingly, creating a dependence between these features. This type of dependence is referred to as multicolinearity. . | Normality: $y$ given any $X$ comes from a normal distribution. . More specifically, we say that $y in mathcal{N}(X^T beta, sigma^2 I)$, which also implies that the distribution of the residuals is normal and independent of $X$. Furthermore, this means we can consider linear regression as predicting the mean of a normal distribution. . | $$y = mathbb{E}[y|X] = mu(x) = X^T beta + epsilon$$ . Homoscedasticity: The variance of residual is the same for any value of $X$ . This is a direct corollary from $y in mathcal{N}(X^T beta, sigma^2 I)$. $ sigma^2 I$ implies that the variance of the distribution is not dependent on $X$, and each $X$ has the same variance. Heteroscedasticity often implies a lack of linearity, independence, or normality. It&#39;s a hint that there are complexities in the data that cannot be represented by your current model. If your linear model has variance within the error terms, this implies that you should probably explore other methodologies, as a classic linear model was not able to properly capture $ mathbb{E}[y|X]$. . | These assumptions are quite rigid for the real world. Many datasets and/or problem spaces do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can comparably perform the regression task without such rigid assumptions? Two common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. . Linear regression is the simplest way of solving a regression problem, and it&#39;s often a good idea to start simple. . | Interpretability: The decisions of the model can be explained with respect to $X$ and $y$. . Linear regression fits an exact equation known apriori. When the model spits out $ beta_2$, that means that, according to the model, the feature $X_2$ contributes $ beta_2 X_2$ to the outcome. . | In many scenarios, the learned $ beta$ coefficients are more valuable than the actual prediction. Let&#39;s say you (unadvisably so) devised a linear model for predicting the expected salary of an individual, and that model (also unadvisably so) contained sensitive factors like race or gender. If the model learns high coefficients with respect to those features, well, you have a sexist and/or racist model! While you probably shouldn&#39;t have even attempted building this model in the first place, at least it was fully transparent so you know it is unfair and hence won&#39;t deploy it (I hope). . Today, we are going to stick with the notion that transparency is of the utmost importance. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, contrary to what you may have heard, these models don&#39;t need to be exactly linear as described above. . Generalized Linear Models . Generalized Linear Models (GLMs) are exactly what they sound like, a generalization of the definitions described above. Technically, linear regression is a GLM. However, the set of potential GLMs describe a much wider array of functions. . GLMs loosen the constraints of normality. Now the mean of the distribution connecting $y$ to $X$ can represent an expectation from any distribution in the exponential family1. This includes quite a lot of extra distributions, such as normal, bernoulli, poisson, and more! . However, not all of these distributions range from $- infty$ to $+ infty$, which is necessary for solving regression problems in the linear form $y = X^T beta$. In order for us to maintain a linear model, we transform the mean of the distribution ( $ mu(x) = mathbb{E}[y|X]$ ) using a specified function $g$. $g$ is called the link function. . $$g( mu(x)) = X^T beta + epsilon$$ . Observe that this also loosens the constraints of linearity. Recall that $ mu(x)$ is just our expectation for $y$ given $X$. So, if we substitute back $y$, and take the inverse to get rid of $g$ in relationship to $y$, we get: . $$y = g^{-1}(X^T beta) + epsilon$$ . Hence the relationship between $y$ and $X$ is no longer required to be exactly linear. However, the relationship between the resulting $g(y)$ transformed by the link function and $X$ must be linear. . Note that this clarification implies that L $g$ is the identity function $g( mu) = mu$, then the GLM is just normal linear regression. . Logistic Regression . Logistic regression is actually a GLM! Let&#39;s say we&#39;re trying to predict some binary variable, which means that $y$ will be either 0 or 1. This can be described by a bernoulli distribution. In order to squash the regression results to be between 0 and 1, we use the logit link function $g( mu) = log( frac{ mu}{1 - mu})$. . And GLMs are as easy as that! Given the expected distribution of your problem, you determine a link function that will properly transform your data to the right space. And then you can fit a simple linear model: $y = X^T beta$. . Relating to Neural Networks . class Regression(tf.keras.Model): def __init__( self, name=None, style=&#39;linear&#39; ): super().__init__(name=name) # linear regression uses the identity as the link function # and the inverse of the identity is a linear activation if style.lower() == &#39;linear&#39;: activation = &#39;linear&#39; # logistic regression uses the logit link function, and the # inverse of logit is a sigmoid activation function elif style.lower() == &#39;logistic&#39;: activation = &#39;sigmoid&#39; # no other options are supported else: raise ValueError(&#39;input style only supports two options: linear or logistic&#39;) # pass input directly to the output layer no hidden layers self.output_layer = tf.keras.layers.Dense(1, activation=activation) def call(self, x, training=None): return self.output_layer(x) . model = Regression(style=&#39;logistic&#39;) model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=10, batch_size=32 ) . Train on 100000 samples Epoch 1/10 100000/100000 [==============================] - 4s 36us/sample - loss: 0.1662 - mean_absolute_error: 0.3909 Epoch 2/10 100000/100000 [==============================] - 3s 31us/sample - loss: 0.0964 - mean_absolute_error: 0.2793 Epoch 3/10 100000/100000 [==============================] - 3s 32us/sample - loss: 0.0736 - mean_absolute_error: 0.2278 Epoch 4/10 100000/100000 [==============================] - 3s 33us/sample - loss: 0.0617 - mean_absolute_error: 0.1974 Epoch 5/10 100000/100000 [==============================] - 3s 32us/sample - loss: 0.0544 - mean_absolute_error: 0.1771 Epoch 6/10 100000/100000 [==============================] - 3s 32us/sample - loss: 0.0493 - mean_absolute_error: 0.1627 Epoch 7/10 100000/100000 [==============================] - 3s 31us/sample - loss: 0.0455 - mean_absolute_error: 0.1515 Epoch 8/10 100000/100000 [==============================] - 3s 31us/sample - loss: 0.0425 - mean_absolute_error: 0.1425 Epoch 9/10 100000/100000 [==============================] - 3s 32us/sample - loss: 0.0400 - mean_absolute_error: 0.1351 Epoch 10/10 100000/100000 [==============================] - 3s 31us/sample - loss: 0.0380 - mean_absolute_error: 0.1289 . Generalized Additive Models . Generalized Additive Models (GAMs) take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling: . Elimination of the linearity restriction. . GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth (differentiable everywhere). . | A change to the independence criterion. . Linear models make the assumption that $X_i$ and $X_j$ are independent forall $i neq j$. Additive models don&#39;t have this property, however we assume that which features interact are known apriori. Essentially, this just means that when you set up a GAM, you are allowed to have $h_i(X_i,X_j, cdots)$ be a part of the equation. . | $$g( mu(X)) = beta_0 + beta_1 h_1(X_1) + beta_2 h_2(X_2, X_3) + cdots + beta_N h_M(X_N) + epsilon$$ . from pygam import LinearGam from pygam import s as spline gam = LinearGAM(spline(0) + spline(1) + spline(2)).fit(X, y) . Neural Additive Models . # relu helps learn more jagged functions if necessary class MLP(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) self.l1 = tf.keras.layers.Dense(8, activation=&#39;relu&#39;) self.l2 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = tf.keras.layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Neural Additive Model for n features class NAM(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features self.components = [MLP() for i in range(n_features)] self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): individual_features = tf.split(x, self.n_features, axis=1) components = [] for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) components = tf.concat(components, axis=1) return self.linear_combination(components) . model = NAM(n_features=3) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=10, batch_size=32, ) . Train on 1000000 samples Epoch 1/10 1000000/1000000 [==============================] - 60s 60us/sample - loss: 414.4350 - mean_absolute_error: 10.9988 Epoch 2/10 1000000/1000000 [==============================] - 57s 57us/sample - loss: 120.1113 - mean_absolute_error: 4.9874 Epoch 3/10 1000000/1000000 [==============================] - 55s 55us/sample - loss: 74.6649 - mean_absolute_error: 3.6563 Epoch 4/10 1000000/1000000 [==============================] - 54s 54us/sample - loss: 59.5442 - mean_absolute_error: 3.0831 Epoch 5/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 51.1148 - mean_absolute_error: 2.6067 Epoch 6/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 46.4825 - mean_absolute_error: 2.3151 Epoch 7/10 1000000/1000000 [==============================] - 55s 55us/sample - loss: 43.2321 - mean_absolute_error: 2.1660 Epoch 8/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 41.0939 - mean_absolute_error: 2.0531 Epoch 9/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 39.4331 - mean_absolute_error: 1.9772 Epoch 10/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 37.6433 - mean_absolute_error: 1.9064 . &lt;tensorflow.python.keras.callbacks.History at 0x21904d6d948&gt; . Neural Additive Models with Priors . class CompositionNetwork(tf.keras.Model): def __init__( self, n_features, priors: dict, name=None ): super().__init__(name=name) self.n_features = n_features self.components = [MLP() for i in range(n_features)] self.compose = MLP() @tf.function def call(self, x, training=None): individual_features = tf.split(x, self.n_features, axis=1) components = [] for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) components = tf.concat(components, axis=1) return self.compose(components) . model = CompositionNetwork(n_features=3) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=20, batch_size=32, ) . Train on 1000000 samples Epoch 1/20 1000000/1000000 [==============================] - 69s 69us/sample - loss: 415.1240 - mean_absolute_error: 12.1228 Epoch 2/20 1000000/1000000 [==============================] - 62s 62us/sample - loss: 220.7856 - mean_absolute_error: 8.1069 Epoch 3/20 1000000/1000000 [==============================] - 67s 67us/sample - loss: 197.5810 - mean_absolute_error: 7.4592 Epoch 4/20 1000000/1000000 [==============================] - 66s 66us/sample - loss: 186.9777 - mean_absolute_error: 7.0505 Epoch 5/20 1000000/1000000 [==============================] - 68s 68us/sample - loss: 181.8773 - mean_absolute_error: 6.8436 Epoch 6/20 1000000/1000000 [==============================] - 67s 67us/sample - loss: 184.4130 - mean_absolute_error: 6.7830 Epoch 7/20 1000000/1000000 [==============================] - 64s 64us/sample - loss: 207.3115 - mean_absolute_error: 7.1753 Epoch 8/20 1000000/1000000 [==============================] - 65s 65us/sample - loss: 179.4194 - mean_absolute_error: 6.6264 Epoch 9/20 1000000/1000000 [==============================] - 65s 65us/sample - loss: 162.3328 - mean_absolute_error: 6.2314 Epoch 10/20 1000000/1000000 [==============================] - 66s 66us/sample - loss: 163.3648 - mean_absolute_error: 6.1220 Epoch 11/20 1000000/1000000 [==============================] - 70s 70us/sample - loss: 170.9645 - mean_absolute_error: 6.3206 Epoch 12/20 1000000/1000000 [==============================] - 67s 67us/sample - loss: 182.8074 - mean_absolute_error: 6.4891 Epoch 13/20 1000000/1000000 [==============================] - 65s 65us/sample - loss: 181.3958 - mean_absolute_error: 6.4113 Epoch 14/20 1000000/1000000 [==============================] - 67s 67us/sample - loss: 174.4527 - mean_absolute_error: 6.1452 Epoch 15/20 1000000/1000000 [==============================] - 64s 64us/sample - loss: 170.4005 - mean_absolute_error: 6.0394 Epoch 16/20 1000000/1000000 [==============================] - 64s 64us/sample - loss: 170.2491 - mean_absolute_error: 6.2036 Epoch 17/20 1000000/1000000 [==============================] - 64s 64us/sample - loss: 135.3357 - mean_absolute_error: 5.2641 Epoch 18/20 1000000/1000000 [==============================] - 63s 63us/sample - loss: 96.6227 - mean_absolute_error: 4.3672 Epoch 19/20 1000000/1000000 [==============================] - 63s 63us/sample - loss: 76.7207 - mean_absolute_error: 3.8869 Epoch 20/20 1000000/1000000 [==============================] - 62s 62us/sample - loss: 69.4081 - mean_absolute_error: 3.6303 . col_desc = [ &#39;CRIM&#39;, &#39;per capita crime rate by town&#39;, &#39;ZN&#39;, &#39;proportion of residential land zoned for lots over 25,000 sq.ft.&#39;, &#39;INDUS&#39;, &#39;proportion of non-retail business acres per town&#39;, &#39;CHAS&#39;, &#39;Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)&#39;, &#39;NOX&#39;, &#39;nitric oxides concentration (parts per 10 million)&#39;, &#39;RM&#39;, &#39;average number of rooms per dwelling&#39;, &#39;AGE&#39;, &#39;proportion of owner-occupied units built prior to 1940&#39;, &#39;DIS&#39;, &#39;weighted distances to five Boston employment centres&#39;, &#39;RAD&#39;, &#39;index of accessibility to radial highways&#39;, &#39;TAX&#39;, &#39;full-value property-tax rate per $10,000&#39;, &#39;PTRATIO&#39;, &#39;pupil-teacher ratio by town&#39;, &#39;B&#39;, &#39;1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town&#39;, &#39;LSTAT&#39;, &#39;% lower status of the population&#39;, &#39;MEDV&#39;, &quot;Median value of owner-occupied homes in $1000&#39;s&quot;, ] cols = [col_desc[i] for i in range(0,len(col_desc),2)] print(cols) dataframe = pd.read_csv(&quot;data/housing.csv&quot;, delim_whitespace=True, header=None) dataset = dataframe.values # split into input (X) and output (Y) variables good_cols = [i for i in range(len(cols)) if cols[i] not in [&#39;RAD&#39;,&#39;CHAS&#39;,&#39;MEDV&#39;]] X = dataset[:,good_cols] y = dataset[:,13] . [&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;, &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;, &#39;MEDV&#39;] . model = CompositionNetwork(n_features=len(good_cols)) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X, y, epochs=200, batch_size=32, ) . Train on 506 samples Epoch 1/200 506/506 [==============================] - 3s 6ms/sample - loss: 284.5718 - mean_absolute_error: 14.1215 Epoch 2/200 506/506 [==============================] - 0s 154us/sample - loss: 106.8002 - mean_absolute_error: 7.8904 Epoch 3/200 506/506 [==============================] - 0s 152us/sample - loss: 83.2826 - mean_absolute_error: 6.6863 Epoch 4/200 506/506 [==============================] - 0s 150us/sample - loss: 77.0937 - mean_absolute_error: 6.2208 Epoch 5/200 506/506 [==============================] - 0s 148us/sample - loss: 73.6962 - mean_absolute_error: 6.2673 Epoch 6/200 506/506 [==============================] - 0s 142us/sample - loss: 71.2683 - mean_absolute_error: 6.0643 Epoch 7/200 506/506 [==============================] - 0s 148us/sample - loss: 69.1138 - mean_absolute_error: 5.7704 Epoch 8/200 506/506 [==============================] - 0s 142us/sample - loss: 67.5688 - mean_absolute_error: 5.7927 Epoch 9/200 506/506 [==============================] - 0s 146us/sample - loss: 67.0902 - mean_absolute_error: 5.7145 Epoch 10/200 506/506 [==============================] - 0s 144us/sample - loss: 66.8069 - mean_absolute_error: 5.7467 Epoch 11/200 506/506 [==============================] - 0s 146us/sample - loss: 65.8485 - mean_absolute_error: 5.6075 Epoch 12/200 506/506 [==============================] - 0s 140us/sample - loss: 65.1186 - mean_absolute_error: 5.5677 Epoch 13/200 506/506 [==============================] - 0s 144us/sample - loss: 64.5197 - mean_absolute_error: 5.6530 Epoch 14/200 506/506 [==============================] - 0s 146us/sample - loss: 63.3498 - mean_absolute_error: 5.4881 Epoch 15/200 506/506 [==============================] - 0s 152us/sample - loss: 63.5635 - mean_absolute_error: 5.7356 Epoch 16/200 506/506 [==============================] - 0s 142us/sample - loss: 62.7282 - mean_absolute_error: 5.3989 Epoch 17/200 506/506 [==============================] - 0s 148us/sample - loss: 59.3206 - mean_absolute_error: 5.3774 Epoch 18/200 506/506 [==============================] - 0s 146us/sample - loss: 57.6978 - mean_absolute_error: 5.2011 Epoch 19/200 506/506 [==============================] - 0s 146us/sample - loss: 54.6234 - mean_absolute_error: 5.1952 Epoch 20/200 506/506 [==============================] - 0s 148us/sample - loss: 51.1685 - mean_absolute_error: 4.8725 Epoch 21/200 506/506 [==============================] - 0s 154us/sample - loss: 45.9268 - mean_absolute_error: 4.5724 Epoch 22/200 506/506 [==============================] - 0s 146us/sample - loss: 40.8115 - mean_absolute_error: 4.3565 Epoch 23/200 506/506 [==============================] - 0s 160us/sample - loss: 37.2887 - mean_absolute_error: 4.2512 Epoch 24/200 506/506 [==============================] - 0s 162us/sample - loss: 34.0779 - mean_absolute_error: 4.0456 Epoch 25/200 506/506 [==============================] - 0s 166us/sample - loss: 31.5553 - mean_absolute_error: 4.0420 Epoch 26/200 506/506 [==============================] - 0s 162us/sample - loss: 29.5549 - mean_absolute_error: 3.8871 Epoch 27/200 506/506 [==============================] - 0s 158us/sample - loss: 27.6800 - mean_absolute_error: 3.7887 Epoch 28/200 506/506 [==============================] - 0s 162us/sample - loss: 26.1620 - mean_absolute_error: 3.7280 Epoch 29/200 506/506 [==============================] - 0s 146us/sample - loss: 27.2214 - mean_absolute_error: 3.7117 Epoch 30/200 506/506 [==============================] - 0s 140us/sample - loss: 23.8029 - mean_absolute_error: 3.5556 Epoch 31/200 506/506 [==============================] - 0s 142us/sample - loss: 23.2277 - mean_absolute_error: 3.4745 Epoch 32/200 506/506 [==============================] - 0s 138us/sample - loss: 21.7998 - mean_absolute_error: 3.4360 Epoch 33/200 506/506 [==============================] - 0s 140us/sample - loss: 21.2720 - mean_absolute_error: 3.3786 Epoch 34/200 506/506 [==============================] - 0s 138us/sample - loss: 21.2191 - mean_absolute_error: 3.3068 Epoch 35/200 506/506 [==============================] - 0s 140us/sample - loss: 20.6447 - mean_absolute_error: 3.2692 Epoch 36/200 506/506 [==============================] - 0s 140us/sample - loss: 20.1306 - mean_absolute_error: 3.2875 Epoch 37/200 506/506 [==============================] - 0s 136us/sample - loss: 19.5315 - mean_absolute_error: 3.1761 Epoch 38/200 506/506 [==============================] - 0s 142us/sample - loss: 19.0000 - mean_absolute_error: 3.1127 Epoch 39/200 506/506 [==============================] - 0s 140us/sample - loss: 19.0868 - mean_absolute_error: 3.1697 Epoch 40/200 506/506 [==============================] - 0s 130us/sample - loss: 18.4293 - mean_absolute_error: 3.0585 Epoch 41/200 506/506 [==============================] - 0s 142us/sample - loss: 17.6713 - mean_absolute_error: 3.0097 Epoch 42/200 506/506 [==============================] - 0s 140us/sample - loss: 17.7301 - mean_absolute_error: 2.9764 Epoch 43/200 506/506 [==============================] - 0s 150us/sample - loss: 18.2955 - mean_absolute_error: 3.0626 Epoch 44/200 506/506 [==============================] - 0s 146us/sample - loss: 17.3982 - mean_absolute_error: 2.9361 Epoch 45/200 506/506 [==============================] - 0s 146us/sample - loss: 16.9965 - mean_absolute_error: 2.9388 Epoch 46/200 506/506 [==============================] - 0s 158us/sample - loss: 18.4910 - mean_absolute_error: 3.0468 Epoch 47/200 506/506 [==============================] - 0s 154us/sample - loss: 16.6962 - mean_absolute_error: 2.8660 Epoch 48/200 506/506 [==============================] - 0s 132us/sample - loss: 16.7205 - mean_absolute_error: 2.9524 Epoch 49/200 506/506 [==============================] - 0s 125us/sample - loss: 15.7471 - mean_absolute_error: 2.7926 Epoch 50/200 506/506 [==============================] - 0s 152us/sample - loss: 15.5781 - mean_absolute_error: 2.7657 Epoch 51/200 506/506 [==============================] - 0s 160us/sample - loss: 15.1151 - mean_absolute_error: 2.7165 Epoch 52/200 506/506 [==============================] - 0s 166us/sample - loss: 14.8675 - mean_absolute_error: 2.6937 Epoch 53/200 506/506 [==============================] - 0s 158us/sample - loss: 14.7288 - mean_absolute_error: 2.6704 Epoch 54/200 506/506 [==============================] - 0s 162us/sample - loss: 14.7882 - mean_absolute_error: 2.7213 Epoch 55/200 506/506 [==============================] - 0s 156us/sample - loss: 14.9309 - mean_absolute_error: 2.7364 Epoch 56/200 506/506 [==============================] - 0s 156us/sample - loss: 14.1411 - mean_absolute_error: 2.6259 Epoch 57/200 506/506 [==============================] - 0s 176us/sample - loss: 14.1436 - mean_absolute_error: 2.6354 Epoch 58/200 506/506 [==============================] - 0s 162us/sample - loss: 14.8537 - mean_absolute_error: 2.7416 Epoch 59/200 506/506 [==============================] - 0s 144us/sample - loss: 14.4948 - mean_absolute_error: 2.7461 Epoch 60/200 506/506 [==============================] - 0s 154us/sample - loss: 14.3075 - mean_absolute_error: 2.7297 Epoch 61/200 506/506 [==============================] - 0s 150us/sample - loss: 13.8793 - mean_absolute_error: 2.6099 Epoch 62/200 506/506 [==============================] - 0s 146us/sample - loss: 13.3951 - mean_absolute_error: 2.6000 Epoch 63/200 506/506 [==============================] - 0s 148us/sample - loss: 13.7140 - mean_absolute_error: 2.6342 Epoch 64/200 506/506 [==============================] - 0s 150us/sample - loss: 13.4464 - mean_absolute_error: 2.6144 Epoch 65/200 506/506 [==============================] - 0s 146us/sample - loss: 13.5442 - mean_absolute_error: 2.5960 Epoch 66/200 506/506 [==============================] - 0s 136us/sample - loss: 13.6083 - mean_absolute_error: 2.6289 Epoch 67/200 506/506 [==============================] - 0s 138us/sample - loss: 12.8463 - mean_absolute_error: 2.5287 Epoch 68/200 506/506 [==============================] - 0s 148us/sample - loss: 12.9276 - mean_absolute_error: 2.5286 Epoch 69/200 506/506 [==============================] - 0s 146us/sample - loss: 12.6834 - mean_absolute_error: 2.5698 Epoch 70/200 506/506 [==============================] - 0s 136us/sample - loss: 12.8807 - mean_absolute_error: 2.5348 Epoch 71/200 506/506 [==============================] - 0s 132us/sample - loss: 12.7141 - mean_absolute_error: 2.5096 Epoch 72/200 506/506 [==============================] - 0s 138us/sample - loss: 12.5562 - mean_absolute_error: 2.5218 Epoch 73/200 506/506 [==============================] - 0s 140us/sample - loss: 12.7998 - mean_absolute_error: 2.5233 Epoch 74/200 506/506 [==============================] - 0s 144us/sample - loss: 12.3398 - mean_absolute_error: 2.4908 Epoch 75/200 506/506 [==============================] - 0s 160us/sample - loss: 12.3245 - mean_absolute_error: 2.4802 Epoch 76/200 506/506 [==============================] - 0s 160us/sample - loss: 12.0849 - mean_absolute_error: 2.4890 Epoch 77/200 506/506 [==============================] - 0s 166us/sample - loss: 12.8547 - mean_absolute_error: 2.5795 Epoch 78/200 506/506 [==============================] - 0s 160us/sample - loss: 12.2341 - mean_absolute_error: 2.4798 Epoch 79/200 506/506 [==============================] - 0s 172us/sample - loss: 12.0337 - mean_absolute_error: 2.4562 Epoch 80/200 506/506 [==============================] - 0s 158us/sample - loss: 12.2085 - mean_absolute_error: 2.4700 Epoch 81/200 506/506 [==============================] - 0s 142us/sample - loss: 12.5064 - mean_absolute_error: 2.5012 Epoch 82/200 506/506 [==============================] - 0s 146us/sample - loss: 12.0186 - mean_absolute_error: 2.4664 Epoch 83/200 506/506 [==============================] - 0s 136us/sample - loss: 11.8732 - mean_absolute_error: 2.4196 Epoch 84/200 506/506 [==============================] - 0s 146us/sample - loss: 11.8344 - mean_absolute_error: 2.4264 Epoch 85/200 506/506 [==============================] - 0s 142us/sample - loss: 11.6108 - mean_absolute_error: 2.4055 Epoch 86/200 506/506 [==============================] - 0s 156us/sample - loss: 11.7611 - mean_absolute_error: 2.4075 Epoch 87/200 506/506 [==============================] - 0s 142us/sample - loss: 12.0273 - mean_absolute_error: 2.4634 Epoch 88/200 506/506 [==============================] - 0s 146us/sample - loss: 11.7313 - mean_absolute_error: 2.4098 Epoch 89/200 506/506 [==============================] - 0s 140us/sample - loss: 11.7430 - mean_absolute_error: 2.4233 Epoch 90/200 506/506 [==============================] - 0s 144us/sample - loss: 11.2090 - mean_absolute_error: 2.3610 Epoch 91/200 506/506 [==============================] - 0s 154us/sample - loss: 11.7457 - mean_absolute_error: 2.4049 Epoch 92/200 506/506 [==============================] - 0s 154us/sample - loss: 11.2949 - mean_absolute_error: 2.3412 Epoch 93/200 506/506 [==============================] - 0s 196us/sample - loss: 11.3500 - mean_absolute_error: 2.3393 Epoch 94/200 506/506 [==============================] - 0s 146us/sample - loss: 11.5591 - mean_absolute_error: 2.4048 Epoch 95/200 506/506 [==============================] - 0s 152us/sample - loss: 11.6526 - mean_absolute_error: 2.3917 Epoch 96/200 506/506 [==============================] - 0s 140us/sample - loss: 11.8056 - mean_absolute_error: 2.4546 Epoch 97/200 506/506 [==============================] - 0s 136us/sample - loss: 11.2527 - mean_absolute_error: 2.3588 Epoch 98/200 506/506 [==============================] - 0s 138us/sample - loss: 11.0752 - mean_absolute_error: 2.3329 Epoch 99/200 506/506 [==============================] - 0s 156us/sample - loss: 10.8814 - mean_absolute_error: 2.3203 Epoch 100/200 506/506 [==============================] - 0s 156us/sample - loss: 10.9141 - mean_absolute_error: 2.2989 Epoch 101/200 506/506 [==============================] - 0s 152us/sample - loss: 10.8060 - mean_absolute_error: 2.2926 Epoch 102/200 506/506 [==============================] - 0s 158us/sample - loss: 10.8538 - mean_absolute_error: 2.3294 Epoch 103/200 506/506 [==============================] - 0s 170us/sample - loss: 11.1187 - mean_absolute_error: 2.3482 Epoch 104/200 506/506 [==============================] - 0s 172us/sample - loss: 10.9968 - mean_absolute_error: 2.3189 Epoch 105/200 506/506 [==============================] - 0s 150us/sample - loss: 10.7267 - mean_absolute_error: 2.2873 Epoch 106/200 506/506 [==============================] - 0s 146us/sample - loss: 11.9054 - mean_absolute_error: 2.4827 Epoch 107/200 506/506 [==============================] - 0s 138us/sample - loss: 12.6829 - mean_absolute_error: 2.5962 Epoch 108/200 506/506 [==============================] - 0s 142us/sample - loss: 11.1896 - mean_absolute_error: 2.3437 Epoch 109/200 506/506 [==============================] - 0s 148us/sample - loss: 10.5132 - mean_absolute_error: 2.2883 Epoch 110/200 506/506 [==============================] - 0s 146us/sample - loss: 11.2757 - mean_absolute_error: 2.3903 Epoch 111/200 506/506 [==============================] - 0s 144us/sample - loss: 10.8401 - mean_absolute_error: 2.3644 Epoch 112/200 506/506 [==============================] - 0s 140us/sample - loss: 10.4066 - mean_absolute_error: 2.2819 Epoch 113/200 506/506 [==============================] - 0s 138us/sample - loss: 10.3638 - mean_absolute_error: 2.2715 Epoch 114/200 506/506 [==============================] - 0s 158us/sample - loss: 10.7341 - mean_absolute_error: 2.2989 Epoch 115/200 506/506 [==============================] - 0s 150us/sample - loss: 10.1222 - mean_absolute_error: 2.2417 Epoch 116/200 506/506 [==============================] - 0s 138us/sample - loss: 10.2689 - mean_absolute_error: 2.2642 Epoch 117/200 506/506 [==============================] - 0s 134us/sample - loss: 10.5109 - mean_absolute_error: 2.3183 Epoch 118/200 506/506 [==============================] - 0s 144us/sample - loss: 11.2472 - mean_absolute_error: 2.3599 Epoch 119/200 506/506 [==============================] - 0s 142us/sample - loss: 10.4118 - mean_absolute_error: 2.3070 Epoch 120/200 506/506 [==============================] - 0s 150us/sample - loss: 10.3122 - mean_absolute_error: 2.2643 Epoch 121/200 506/506 [==============================] - 0s 144us/sample - loss: 9.8951 - mean_absolute_error: 2.2331 Epoch 122/200 506/506 [==============================] - 0s 136us/sample - loss: 10.0303 - mean_absolute_error: 2.2423 Epoch 123/200 506/506 [==============================] - 0s 136us/sample - loss: 9.8716 - mean_absolute_error: 2.2317 Epoch 124/200 506/506 [==============================] - 0s 150us/sample - loss: 10.1831 - mean_absolute_error: 2.2928 Epoch 125/200 506/506 [==============================] - 0s 162us/sample - loss: 10.0755 - mean_absolute_error: 2.2492 Epoch 126/200 506/506 [==============================] - 0s 184us/sample - loss: 10.4830 - mean_absolute_error: 2.3191 Epoch 127/200 506/506 [==============================] - 0s 166us/sample - loss: 10.3321 - mean_absolute_error: 2.2831 Epoch 128/200 506/506 [==============================] - 0s 176us/sample - loss: 10.0301 - mean_absolute_error: 2.2731 Epoch 129/200 506/506 [==============================] - 0s 184us/sample - loss: 10.1473 - mean_absolute_error: 2.2691 Epoch 130/200 506/506 [==============================] - 0s 265us/sample - loss: 11.0688 - mean_absolute_error: 2.4236 Epoch 131/200 506/506 [==============================] - 0s 172us/sample - loss: 9.4944 - mean_absolute_error: 2.2202 Epoch 132/200 506/506 [==============================] - 0s 144us/sample - loss: 9.9102 - mean_absolute_error: 2.2519 Epoch 133/200 506/506 [==============================] - 0s 150us/sample - loss: 9.9498 - mean_absolute_error: 2.2529 Epoch 134/200 506/506 [==============================] - 0s 150us/sample - loss: 10.2303 - mean_absolute_error: 2.2758 Epoch 135/200 506/506 [==============================] - 0s 138us/sample - loss: 9.8809 - mean_absolute_error: 2.2501 Epoch 136/200 506/506 [==============================] - 0s 134us/sample - loss: 11.1273 - mean_absolute_error: 2.4454 Epoch 137/200 506/506 [==============================] - 0s 146us/sample - loss: 9.5245 - mean_absolute_error: 2.2263 Epoch 138/200 506/506 [==============================] - 0s 146us/sample - loss: 9.7588 - mean_absolute_error: 2.2318 Epoch 139/200 506/506 [==============================] - 0s 148us/sample - loss: 9.6740 - mean_absolute_error: 2.2235 Epoch 140/200 506/506 [==============================] - 0s 172us/sample - loss: 9.5211 - mean_absolute_error: 2.2411 Epoch 141/200 506/506 [==============================] - 0s 134us/sample - loss: 9.8688 - mean_absolute_error: 2.2238 Epoch 142/200 506/506 [==============================] - 0s 136us/sample - loss: 9.8859 - mean_absolute_error: 2.2456 Epoch 143/200 506/506 [==============================] - 0s 142us/sample - loss: 10.3223 - mean_absolute_error: 2.3180 Epoch 144/200 506/506 [==============================] - 0s 136us/sample - loss: 9.9758 - mean_absolute_error: 2.2301 Epoch 145/200 506/506 [==============================] - 0s 142us/sample - loss: 9.2499 - mean_absolute_error: 2.1855 Epoch 146/200 506/506 [==============================] - 0s 150us/sample - loss: 9.7836 - mean_absolute_error: 2.1976 Epoch 147/200 506/506 [==============================] - 0s 134us/sample - loss: 9.4531 - mean_absolute_error: 2.2176 Epoch 148/200 506/506 [==============================] - 0s 136us/sample - loss: 9.5209 - mean_absolute_error: 2.1977 Epoch 149/200 506/506 [==============================] - 0s 138us/sample - loss: 9.8911 - mean_absolute_error: 2.2927 Epoch 150/200 506/506 [==============================] - 0s 150us/sample - loss: 9.8702 - mean_absolute_error: 2.2659 Epoch 151/200 506/506 [==============================] - 0s 172us/sample - loss: 10.4626 - mean_absolute_error: 2.3876 Epoch 152/200 506/506 [==============================] - 0s 164us/sample - loss: 9.8732 - mean_absolute_error: 2.2590 Epoch 153/200 506/506 [==============================] - 0s 152us/sample - loss: 10.2913 - mean_absolute_error: 2.3278 Epoch 154/200 506/506 [==============================] - 0s 138us/sample - loss: 9.4127 - mean_absolute_error: 2.2319 Epoch 155/200 506/506 [==============================] - 0s 170us/sample - loss: 8.9786 - mean_absolute_error: 2.1463 Epoch 156/200 506/506 [==============================] - 0s 168us/sample - loss: 9.2949 - mean_absolute_error: 2.1815 Epoch 157/200 506/506 [==============================] - 0s 168us/sample - loss: 9.1320 - mean_absolute_error: 2.1530 Epoch 158/200 506/506 [==============================] - 0s 158us/sample - loss: 9.4193 - mean_absolute_error: 2.2198 Epoch 159/200 506/506 [==============================] - 0s 140us/sample - loss: 9.4301 - mean_absolute_error: 2.2152 Epoch 160/200 506/506 [==============================] - 0s 150us/sample - loss: 9.1210 - mean_absolute_error: 2.1921 Epoch 161/200 506/506 [==============================] - 0s 144us/sample - loss: 8.9083 - mean_absolute_error: 2.1579 Epoch 162/200 506/506 [==============================] - 0s 152us/sample - loss: 9.0096 - mean_absolute_error: 2.1763 Epoch 163/200 506/506 [==============================] - 0s 148us/sample - loss: 9.1951 - mean_absolute_error: 2.2022 Epoch 164/200 506/506 [==============================] - 0s 156us/sample - loss: 8.7061 - mean_absolute_error: 2.1373 Epoch 165/200 506/506 [==============================] - 0s 156us/sample - loss: 8.8963 - mean_absolute_error: 2.1568 Epoch 166/200 506/506 [==============================] - 0s 144us/sample - loss: 8.9280 - mean_absolute_error: 2.1725 Epoch 167/200 506/506 [==============================] - 0s 146us/sample - loss: 8.6642 - mean_absolute_error: 2.1215 Epoch 168/200 506/506 [==============================] - 0s 138us/sample - loss: 8.9376 - mean_absolute_error: 2.1705 Epoch 169/200 506/506 [==============================] - 0s 156us/sample - loss: 9.2424 - mean_absolute_error: 2.1741 Epoch 170/200 506/506 [==============================] - 0s 146us/sample - loss: 8.9751 - mean_absolute_error: 2.1010 Epoch 171/200 506/506 [==============================] - 0s 148us/sample - loss: 9.0352 - mean_absolute_error: 2.1572 Epoch 172/200 506/506 [==============================] - 0s 148us/sample - loss: 9.6888 - mean_absolute_error: 2.2058 Epoch 173/200 506/506 [==============================] - 0s 136us/sample - loss: 9.1776 - mean_absolute_error: 2.1644 Epoch 174/200 506/506 [==============================] - 0s 152us/sample - loss: 9.3683 - mean_absolute_error: 2.2449 Epoch 175/200 506/506 [==============================] - 0s 144us/sample - loss: 9.1957 - mean_absolute_error: 2.2122 Epoch 176/200 506/506 [==============================] - 0s 162us/sample - loss: 8.4478 - mean_absolute_error: 2.0881 Epoch 177/200 506/506 [==============================] - 0s 160us/sample - loss: 8.5892 - mean_absolute_error: 2.1066 Epoch 178/200 506/506 [==============================] - 0s 146us/sample - loss: 8.3744 - mean_absolute_error: 2.0786 Epoch 179/200 506/506 [==============================] - 0s 172us/sample - loss: 8.7936 - mean_absolute_error: 2.1311 Epoch 180/200 506/506 [==============================] - 0s 156us/sample - loss: 9.4005 - mean_absolute_error: 2.2251 Epoch 181/200 506/506 [==============================] - 0s 146us/sample - loss: 8.3972 - mean_absolute_error: 2.1104 Epoch 182/200 506/506 [==============================] - 0s 174us/sample - loss: 8.5252 - mean_absolute_error: 2.1411 Epoch 183/200 506/506 [==============================] - 0s 164us/sample - loss: 8.7165 - mean_absolute_error: 2.1053 Epoch 184/200 506/506 [==============================] - 0s 178us/sample - loss: 8.8657 - mean_absolute_error: 2.1638 Epoch 185/200 506/506 [==============================] - 0s 148us/sample - loss: 8.2917 - mean_absolute_error: 2.0814 Epoch 186/200 506/506 [==============================] - 0s 150us/sample - loss: 8.3021 - mean_absolute_error: 2.1106 Epoch 187/200 506/506 [==============================] - 0s 146us/sample - loss: 9.6708 - mean_absolute_error: 2.2581 Epoch 188/200 506/506 [==============================] - 0s 148us/sample - loss: 8.5917 - mean_absolute_error: 2.0914 Epoch 189/200 506/506 [==============================] - 0s 144us/sample - loss: 8.7749 - mean_absolute_error: 2.1374 Epoch 190/200 506/506 [==============================] - 0s 148us/sample - loss: 8.5210 - mean_absolute_error: 2.1286 Epoch 191/200 506/506 [==============================] - 0s 148us/sample - loss: 9.6339 - mean_absolute_error: 2.2565 Epoch 192/200 506/506 [==============================] - 0s 132us/sample - loss: 8.8856 - mean_absolute_error: 2.2046 Epoch 193/200 506/506 [==============================] - 0s 146us/sample - loss: 8.8120 - mean_absolute_error: 2.1470 Epoch 194/200 506/506 [==============================] - 0s 152us/sample - loss: 8.2462 - mean_absolute_error: 2.0984 Epoch 195/200 506/506 [==============================] - 0s 148us/sample - loss: 8.8462 - mean_absolute_error: 2.2002 Epoch 196/200 506/506 [==============================] - 0s 144us/sample - loss: 8.5919 - mean_absolute_error: 2.1165 Epoch 197/200 506/506 [==============================] - 0s 148us/sample - loss: 8.2336 - mean_absolute_error: 2.0911 Epoch 198/200 506/506 [==============================] - 0s 154us/sample - loss: 8.1897 - mean_absolute_error: 2.0738 Epoch 199/200 506/506 [==============================] - 0s 144us/sample - loss: 8.2386 - mean_absolute_error: 2.0807 Epoch 200/200 506/506 [==============================] - 0s 160us/sample - loss: 8.9582 - mean_absolute_error: 2.1974 . for i,col in enumerate(good_cols): data = X[:,i] start = data.min() end = data.max() test_x = np.expand_dims(np.linspace(start,end,100,dtype=np.float32),axis=1) pred_y = model.components[i](test_x).numpy() plt.plot(test_x, pred_y, label=&#39;predicted_function&#39;) plt.show() . which just means that an exponential shows up in the probility density function (pdf)↩ | .",
            "url": "https://ryansaxe.com/transparency/2021/01/24/playground.html",
            "relUrl": "/transparency/2021/01/24/playground.html",
            "date": " • Jan 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Designing Transparent Neural Networks",
            "content": "Most systems we interact with are part of some pipeline that integrates Machine Learning (ML). Sometimes we interact with an ML model directly, like Spotify&#39;s recommender system for songs. Other times, this interaction is more detatched; we post a comment on Twitter or Facebook, and this comment is used to train some language model at the respective company. As ML models become more and more prevalent, interpreting and explaining the decisions these models make become increasingly important. . Neural networks are a popular class of machine learning algorithms, which are notorious for being difficult to interpret. They are often referred to as &quot;black boxes&quot; or &quot;opaque&quot;. This blog post will explore a particular type of neural architecture that is inherrantly transparent. However, prior to jumping into that, it&#39;s important to understand the fundamental transparent algorithms, starting from the simplest linear models. . . Note: If you are familiar the relationship between GLMs and NNs, feel free to skip this introductory section. . . Introducing Linear and Additive Models . Linear models are one of the simplest approaches to supervised learning. The general goal of supervised learning is to discover some function $f$ that minimizes an error-term $ epsilon$ given a set of input features $X$ and a corresponding target $y$ such that $y = f(X) + epsilon$. Additionally, the output of a supervised model is often written as $ hat{y} = f(X)$ because $f(X)$ is our best approximation of $y$. . Different algorithms are sufficient for learning different functions. Simple linear models can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_n X_n + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$, and $T$ is the transpose operation, which in this case is basically identical to computing the dot product of two n-dimensional vectors. . Linear Regression . Linear regression is arguably the simplest linear model, and comes with four assumptions: . Linearity: The relationship between $X$ and the mean of $y$ is linear. | Independence: $X_i$ and $X_j$ are linearly independent of eachother for all $i neq j$. | Normality: $y$ given any $X$ comes from a normal distribution. | Homoscedasticity: The variance of the error is the same for any value of $X$. | These assumptions can be nicely described by one math equation: . $$ begin{aligned} y &amp; in mathcal{N}(X^T beta, sigma^2 I) &amp; Rightarrow mathbb{E}[y|X] = mu(X) = X^T beta end{aligned} $$Unfortunately, these assumptions are quite rigid for the real world. Many datasets do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can perform the regression task without such rigid assumptions? The common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. | Little Data: Ordinary Least Squares (OLS) is a closed form solution to linear regression1. | Interpretability: $y$ can be explained with respect to how $X$ interacts with the $ beta$ coefficients. | Today, we are going to stick with the notion that transparency is of the utmost importance, and assume we have a significant amount of data. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, these models don&#39;t need to be exactly linear in order to be interpretable. . Generalized Linear Models (GLMs) . Generalized Linear Model (GLM), introduced in (missing reference), loosen the constraints of normality, linearity, and homoscedasticity described in the previous section via the following specifications: . Random Component: The probability distribution of $y$ (typically belonging to the exponential family2). | Systematic Component: the right side of the equation for predicting $y$ (typically $X^T beta$). | Link Function: A function $g$ that links the systematic component and the random component. | This yields the following general equation for GLMs: . $$g( mathbb{E}[y|X]) = X^T beta + epsilon$$ . Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the corresponding GLM is exactly linear regression! Hence, the functions that GLMs can describe are a superset of the functions linear regression can describe. . Selecting a link function according to the random component is what differentiates GLMs. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(- infty,+ infty)$, as that is the expected range of $X^T beta$. As an example, binary logistic regression assumes the probability distribution of $y$ is a bernoulli distribution. This means that the average of the distribution, $ mu$, is between 0 and 1. We need some function $g: [0,1] rightarrow Reals$, and the logit function is sufficient for this: . $$g( mu) = log( frac{ mu}{1 - mu})$$ . Now, we can fit a simple linear model to $g(y) = X^T beta + epsilon$. Unfortunately, introducing a non-linear transformation to this equation means that Ordinary Least Squares is no longer a reasonable estimation method. Hence, learning $ beta$ requires a different estimation method. Maximum Likelihood Estimation (MLE) estimates the parameters of a probability distribution by maximizing the likelihood that a sample of observed data belongs to that probability distribution. In fact, under the assumptions of simple linear regression, MLE is equivalent to OLS as demonstrated on page 2 of these CMU lecture notes. The specifics of MLE are not necessary for the rest of this blog post, however if you would like to learn more about it, please refer to these Stanford lecture notes. . The Building Blocks of Neural Networks . But where do neural networks come in? Aren&#39;t they incredibly non-linear and opaque, unlike GLMs? Sort of. On the macro-level, NNs and GLMs look very different, but the micro-level tells a different story. Let&#39;s zoom into the inner workings of neural networks and see how they relate to GLMs! . Neural networks are built of components called layers. Layers are built of components called nodes. At their heart, these nodes are computational-message-passing-machines. They receive a set of inputs, perform a computation, and pass the result of that computation to other nodes in the network. These are the building blocks of neural networks. . The first layer of a neural network is called the input layer, because each node passes an input feature to all nodes in the next layer. The last layer of a neural network is called the output layer, and it should represent the output you are trying to predict (this layer has one node in the classic regression case). Lastly, any layers between the input and output layers are called hidden layers. . . In the classic fully-connected feed-forwad neural network, this structure of layers is ordered and connected such that every node $n_j$ in layer $L_i$ receives the output of every node in the predecing layer $L_{i-1}$, does some computation with those outputs, and passes the corresponding output to each node in the succeeding layer $L_{i + 1}$. The image above displays a neural network with $N$ input features, a single hidden layer, and a single output prediction $ hat{y}$. . Each node in layer $L_i$ contains some set of weights ($w$) and a bias ($b$), where the dimension of the weight vector is equal to the number of nodes in layer $L_{i - 1}$. When the node receives the output of all the nodes in the preceding layer, it performs the following computation: $L_{i - 1}^Tw + b$. . This should look familiar! It is quite literally $X^T beta$: the classic computation from linear models on the ouputs of the preceding layer! . However, before this node passes $X^T beta$ to the next layer in the network, it is passed through an activation function $f$. Activation functions often introduce non-linearity to the neural network, similar to link functions in GLMs. The image below isolates a single neuron from the image above, taking input from the previous layer, and making a prediction by transforming the output of the neuron with an activation function $ hat{y} = f(X^T beta)$. . . In fact, observe that if the activation function is invertible, this computation is equivalent to $f^{-1}( hat{y}) = X^T beta$, which is exactly a GLM with link function $f^{-1}$. This demonstrates that the computation of a single node in a neural network is, conceptually, a GLM on the output of the previous layer! . Furthermore, this means that a neural network with zero hidden layers and a linear activation function on the output layer is exactly equivalent to linear regression, as the lack of hidden layers maintains independence. And, if we change the activation function to the inverse of the logit function (this is the sigmoid activation function), this neural network becomes exactly equivalent to logistic regression! The code below is a simple prototype of building linear and logistic regression with Keras, and tests it on a simulated dataset. . class LinearRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a linear activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;linear&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) class LogisticRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a sigmoid activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) . However, It isn&#39;t always as simple as the cases of linear and logistic regression. Not all activation functions are invertible (e.g. ReLU), and hence add non-linearities in ways that are not consistent with GLMs. At the same time, the principal of the link function is to transform $y$ to the proper space, and ReLU does accomplish this under the assumption that $y$ cannot be negative. . There is clearly an intimate connection between neural networks and linear models, as the computational components of neural networks are quite literally non-linear transforms on linear models just like GLMs. So, why are neural networks opaque and GLMs transparent? Let&#39;s look at the math for regression using an arbitrarily defined neural network with $k$ hidden layers. . $$ begin{aligned} L_0&amp; = big [ X_1, X_2, cdots, X_n big ] L_1 &amp; = big [ hspace{0.5em}f_1(L_0^Tw_{1,1} + b_{1,1}), hspace{0.5em}f_1(L_0^Tw_{1,2} + b_{1,2}), cdots big ] &amp; vdots L_k &amp; = big [ hspace{0.5em}f_k(L_{k - 1}^Tw_{k,1} + b_{k,1}), hspace{0.5em}f_k(L_{k - 1}^Tw_{k,2} + b_{k,2}), cdots big ] hat{y} &amp; = L_k^T beta end{aligned} $$Where $w_{i,j}$ and $b_{i,j}$ are the weights and bias of the jth node in layer $L_i$ with activation function $f_i$. . Observe that for every layer after $L_1$, the information passed to nodes in that layer from the preceding layer are dependent on every input feature from the input layer. Hence, the inputs to the node are not linearly independent. This means that the restriction of independence breaks once a single hidden layer is introduced to the network3. This dependency is what creates the opacity of neural networks, not the non-linearity. This can be seen in the next section on Generalized Additive Models, a transparent approach with highly non-linear transforms to the input features, while maintaining some notion of independence. This will shed light on how to design neural networks with control over feature dependence! . Generalized Additive Models (GAMs) . Generalized Additive Models (GAMs), introduced in (Hastie &amp; Tibshirani, 1986), take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling: . Allow non-linearity: GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth4. It also can be non-parametric. . | Allow dependence: Linear models make the assumption that $X_i$ and $X_j$ are linearly independent forall $i neq j$. Additive models don&#39;t have this property, however we assume that which features interact are known apriori. This means that the systematic component can be an equation that contains non-linear feature interaction like $h_k(X_i,X_j, cdots)$. . | Hence, equations for GAMs look like this: . $$g( mu(X)) = beta_0 + beta_1 h_1(X_1) + beta_2 h_2(X_2, X_3) + cdots + beta_m h_m(X_n) + epsilon$$ . Technically, this makes GLMs a special case of GAMs where all functions $h$ are the identity function. . Interpretable Neural Networks . (Hornik et al., 1989) is the original paper suggesting NNs are a type of universal approximator. The theory of this contribution was explored in multi-layer perceptrons in (Pinkus, 1999) and generally formalized by (Balázs, 2001). The theorem can be summarized by: . A neural network with a single hidden layer of infinite width can approximate any continuous function. . This theorem means that neural networks can be used to approximate the feature-wise non-linear, non-parametric, functions ($h_k$ in the previous section) necessary for Generalized Additive Models. Furthermore, neural networks fit via backpropogation can describe a wider set of functions than GAMs fit via backfitting and regularization. This is because continuous functions don&#39;t have to be smooth, while smooth functions have to be continuous. . Above are images of three functions. The first two are piecewise continuous, but not smooth. The last function is a linear combination of the first two. We would like to build a model that can fit the last function, while maintaining feature-wise interpretability such that we can see that it properly learns the first two functions. Unfortunately, it is not reasonable to expect a GAM to fit the non-smooth functions. Below is a GAM with very very low regularization and smoothness penalties. Furthermore, it is trained and tested on the same dataset. This is a strong demonstration that GAMs cannot approach these problems, because they can&#39;t even overfit to the solution. . from pygam import LinearGAM from pygam import s as spline #fit a classic GAM with no regularization or smoothing penalties to try and #let it overfit to non-smooth functions. It still fails! gam = LinearGAM( spline(0,lam=0,penalties=None) + spline(1,lam=0,penalties=None), callbacks=[] ).fit(X, y) . While these fits aren&#39;t terrible in terms of prediction error, that&#39;s not what we care about. We care about learning the proper structure for the functions that explain the relationship between individual features and our output. The above plots are a clear demonstration that GAMs can&#39;t even overfit to provide that. . Luckily, because these functions are continuous, we can use a neural network to approximate them! . Generalized Additive Neural Networks (GANNs) . The trick is to use a different neural network for each individual feature, and add them together just like how GAMs work! By replacing the non-linear, non-parametric, functions in GAMs by neural networks, we get get Generalized Additive Neural Networks (GANNs), introduced in (Potts, 1999). Unfortunately, this contribution did not take off because we didn&#39;t have the technical capacity to train large networks as we do today. Luckily, now it is quite easy to fit such a model. . . GANNs are simply a linear combination of neural networks, where each network only observes a single input feature5. Because each of these networks take a single input feature, and provide a single output feature, it becomes possible to plot a two-dimensional graph where the x-axis is the input feature and the y-axis is the output feature for each network. This graph is hence a fully transparent function describing how the neural network learned to transform the input feature as it contributes, additively, to the prediction. Hence this type of neural architecture is sufficient for creating a model as transparent as the linear models described in this blog postso far. . Below is code for creating a GANN using Keras. As you can see, this model is capable of solving the regression problem while maintaining feature-wise transparency on piecewise continous functions! . from tensorflow.keras import layers, regularizers #define a simple multi-layer perceptron class NN(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) # relu helps learn more jagged functions if necessary. self.l1 = layers.Dense(8, activation=&#39;relu&#39;) # softplus helps smooth between the jagged areas from above # as softplus is referred to as &quot;SmoothRELU&quot; self.l2 = layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Generalized Additive Neural Network for n features class GANN(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features # initialize MLP for each input feature self.components = [NN() for i in range(n_features)] # create final layer for a linear combination of learned components self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): #split up by individual features individual_features = tf.split(x, self.n_features, axis=1) components = [] #apply the proper MLP to each individual feature for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) #concatenate learned components and return linear combination of them components = tf.concat(components, axis=1) return self.linear_combination(components) . model = GANN(n_features=2) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=50, batch_size=32, ) . At first glance, these results look good, but not perfect. The right-most-plot demonstrates this model achieved a near perfect fit of the actual task, while the first two plots look like the Neural Additive Model was capable of learning the general shapes of the functions, but is off on the intercepts for all of them. This should not discount the validity of the model. The corresponding math demonstrates perfectly fitting intercepts of an additive model cannot be guaranteed. . Let $h_i(X_i) = alpha_i + f(X_i)$ where $f(X_i)$ represents all of the aspects of $h(X_i)$ that dependent on $X_i$, and $ alpha_i$ represents the intercept. . $$ begin{aligned} y &amp; = beta_0 + sum_{i=1}^n beta_i h_i(X_i) &amp; = beta_0 + sum_{i=1}^n beta_i ( alpha_i + f_i(X_i)) &amp; = beta_0 + sum_{i=1}^n beta_i alpha_i + sum_{i=1}^n beta_i f_i(X_i) end{aligned} $$The only way to tease apart these intercepts is via $ beta$. Imagine the proper fit of this equation, for every $i$, had $ beta_i = 1$ and $ alpha_i = 2$. In this case, if half of the learned $ alpha_i$s are zero, and the other half are four, that would yield the exact same result for $ sum_{i=1}^n beta_i alpha_i$ as the proper fit. Hence, by way of contradictory example, it is impossible to guarantee learning correct intercepts for the individual components of any additive model. . The plots below are what happens when we simply adjust the learned intercepts for these functions. As you can see, by simply changing the intercept, we are able to show a near perfect fit of these functions, which is the best we can ever hope to do! . This is a clear deomonstration that Generalized Additive Neural Networks are capable of overfitting to piecewise continuous functions while maintaining transparency! . However, this is still just a toy example. The next section delves into Neural Additive Models (NAMs), which are a special variant of GANNs that are designed to be able to fit &quot;jumpy functions&quot;, as this is more reminiscient of real-world data. . Neural Additive Models (NAMs) . (Agarwal et al., 2020) . Generalizing the Computational Graph . This is the fun part. This is where all your creativity can yield an extremely flexible, powerful, and interpretable model! The world is your oyster, and you can use your understanding of neural networks and additive models to design an architecture that accomplishes your goals. . References Hastie, T., &amp; Tibshirani, R. (1986). Generalized Additive Models. Statist. Sci., 1(3), 297–310. https://doi.org/10.1214/ss/1177013604 | Hornik, K., Stinchcombe, M., &amp; White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5), 359–366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 | Pinkus, A. (1999). Approximation theory of the MLP model in neural networks. Acta Numerica, 8, 143–195. https://doi.org/10.1017/S0962492900002919 | Balázs, C. (2001). Approximation with Artificial Neural Networks. https://doi.org/10.1.1.101.2647 | Potts, W. J. E. (1999). Generalized Additive Neural Networks. Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 194–200. https://doi.org/10.1145/312129.312228 | Agarwal, R., Frosst, N., Zhang, X., Caruana, R., &amp; Hinton, G. E. (2020). Neural Additive Models: Interpretable Machine Learning with Neural Nets. | . Footnotes . The closed form solution for OLS is $ beta = (X^TX)^ inv X^Ty$. This requires $X^TX$ to be invertible, which is the case when the elements in $X$ are linearly independent. This is satisfied by our assumption of independence. Without this assumption, there is no closed form solution, and $ beta$ can be approximated by the maximum likelihood estimation function: $min_ beta(y - beta X)^T(y - beta X)$.↩ | The exponential family is a particular family of probability distributions such that their probability density function (PDF) can be writted as: $P(x | theta) = f(x) g( theta) exp Big( eta( theta) centerdot T(x) Big)$, where $f$, $g$, $ eta$, and $T$ are known functions and $ theta in Reals$ is the only parameter to the PDF.↩ | This explains why we could create a model identical to linear and logistic regression by designing a neural network with zero hidden layers.↩ | The &quot;smoothness&quot; of a function is described by the continuity of the derivatives. The set of functions with a smoothness of 0 is equivalent to the set of continuous functions. The set of functions with a smoothness of 1 is the set of continuous functions such that their first derivative is continuous. So on, and so forth. Generally, a function is considered &quot;smooth&quot; if it has &quot;smoothness&quot; of $ infty$. In other words, it is infinitely differentiable.↩ | Technically, this could be fit where the sub-networks take more than a single feature as input, but this comes at a cost of interpretability. It is still possible to explore the relationship between both features and the output, however it becomes high-dimensional, entangled, and hence more difficult to interpret.↩ |",
            "url": "https://ryansaxe.com/transparency/2020/12/01/NAM.html",
            "relUrl": "/transparency/2020/12/01/NAM.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  

  

  

  

  
  

  

  
  

  

  

  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}