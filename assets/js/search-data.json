{
  
    
        "post0": {
            "title": "Neural Networks Aren't So Scary",
            "content": "Neural Networks (NNs) are an extremely popular type of machine learning algorithm known for, theoretically, being able to approximate any continuous function (Hornik et al., 1989). They initially seem daunting, but once you understand the fundamental building blocks of NNs, they really aren&#39;t so scary. This blog post provides an overview of linear models, explains how NNs make computations, and demonstrates that you can understand NNs fairly well with only the math background of linear models. . Linear models are one of the simplest approaches to supervised learning. The general goal of supervised learning is to discover some function $f$ that minimizes an error-term $ epsilon$ given a set of input features $X$ and a corresponding target $y$ such that $y = f(X) + epsilon$. Additionally, the output of a supervised model is often written as $ hat{y} = f(X)$ because $f(X)$ is our best approximation of $y$. . Different algorithms are sufficient for learning different functions. Simple linear models can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_n X_n + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$, and $T$ is the transpose operation, which in this case is basically identical to computing the dot product of two n-dimensional vectors. . Linear Regression . Linear regression is arguably the simplest linear model, and comes with four assumptions: . Linearity: The relationship between $X$ and the mean of $y$ is linear. | Independence: $X_i$ and $X_j$ are linearly independent of eachother for all $i neq j$. | Normality: $y$ given any $X$ comes from a normal distribution. | Homoscedasticity: The variance of the error is the same for any value of $X$. | These assumptions can be nicely described by one math equation: . $$ begin{aligned} y &amp; in mathcal{N}(X^T beta, sigma^2 I) &amp; Rightarrow mathbb{E}[y|X] = mu(X) = X^T beta end{aligned} $$Unfortunately, these assumptions are quite rigid for the real world. Many datasets do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can perform the regression task without such rigid assumptions? The common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. | Little Data: Ordinary Least Squares (OLS) is a closed form solution to linear regression1. | Interpretability: $y$ can be explained with respect to how $X$ interacts with the $ beta$ coefficients. | Generalized Linear Models (GLMs) . Generalized Linear Model (GLM), introduced in (Nelder &amp; Wedderburn, 1972), loosen the constraints of normality, linearity, and homoscedasticity described in the previous section. Furthermore, GLMs break down the problem into three different components: . Random Component: The probability distribution of $y$ (typically belonging to the exponential family2). | Systematic Component: the right side of the equation for predicting $y$ (typically $X^T beta$). | Link Function: A function $g$ that links the systematic component and the random component. | This yields the following general equation for GLMs: . $$g( mathbb{E}[y|X]) = X^T beta + epsilon$$ . Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the corresponding GLM is exactly linear regression! Hence, the functions that GLMs can describe are a superset of the functions linear regression can describe. . Selecting a link function according to the random component is what differentiates GLMs. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(- infty,+ infty)$, as that is the expected range of $X^T beta$. As an example, binary logistic regression assumes the probability distribution of $y$ is a bernoulli distribution. This means that the average of the distribution, $ mu$, is between 0 and 1. We need some function $g: [0,1] rightarrow Reals$, and the logit function is sufficient for this: . $$g( mu) = log( frac{ mu}{1 - mu})$$ . Now, we can fit a simple linear model to $g(y) = X^T beta + epsilon$. Unfortunately, introducing a non-linear transformation to this equation means that Ordinary Least Squares is no longer a reasonable estimation method. Hence, learning $ beta$ requires a different estimation method. Maximum Likelihood Estimation (MLE) estimates the parameters of a probability distribution by maximizing the likelihood that a sample of observed data belongs to that probability distribution. In fact, under the assumptions of simple linear regression, MLE is equivalent to OLS as demonstrated on page 2 of these CMU lecture notes. The specifics of MLE are not necessary for the rest of this blog post, however if you would like to learn more about it, please refer to these Stanford lecture notes. . The Building Blocks of Neural Networks . But where do neural networks come in? Aren&#39;t they incredibly non-linear and opaque, unlike GLMs? Sort of. On the macro-level, NNs and GLMs look very different, but the micro-level tells a different story. Let&#39;s zoom into the inner workings of neural networks and see how they relate to GLMs! . Neural networks are built of components called layers. Layers are built of components called nodes. At their heart, these nodes are computational-message-passing-machines. They receive a set of inputs, perform a computation, and pass the result of that computation to other nodes in the network. These are the building blocks of neural networks. . The first layer of a neural network is called the input layer, because each node passes an input feature to all nodes in the next layer. The last layer of a neural network is called the output layer, and it should represent the output you are trying to predict (this layer has one node in the classic regression case). Lastly, any layers between the input and output layers are called hidden layers. . . In the classic fully-connected feed-forwad neural network, this structure of layers is ordered and connected such that every node $n_j$ in layer $L_i$ receives the output of every node in the predecing layer $L_{i-1}$, does some computation with those outputs, and passes the corresponding output to each node in the succeeding layer $L_{i + 1}$. The image above displays a neural network with $N$ input features, a single hidden layer, and a single output prediction $ hat{y}$. . Each node in layer $L_i$ contains some set of weights ($w$) and a bias ($b$), where the dimension of the weight vector is equal to the number of nodes in layer $L_{i - 1}$. When the node receives the output of all the nodes in the preceding layer, it performs the following computation: $L_{i - 1}^Tw + b$. . This should look familiar! It is quite literally $X^T beta$: the classic computation from linear models on the ouputs of the preceding layer! . However, before this node passes $X^T beta$ to the next layer in the network, it is passed through an activation function $f$. Activation functions often introduce non-linearity to the neural network, similar to link functions in GLMs. The image below isolates a single neuron from the image above, taking input from the previous layer, and making a prediction by transforming the output of the neuron with an activation function $ hat{y} = f(X^T beta)$. . . In fact, observe that if the activation function is invertible, this computation is equivalent to $f^{-1}( hat{y}) = X^T beta$, which is exactly a GLM with link function $f^{-1}$. This demonstrates that the computation of a single node in a neural network is, conceptually, a GLM on the output of the previous layer! . Furthermore, this means that a neural network with zero hidden layers and a linear activation function on the output layer is exactly equivalent to linear regression, as the lack of hidden layers maintains independence. And, if we change the activation function to the inverse of the logit function (this is the sigmoid activation function), this neural network becomes exactly equivalent to logistic regression! The code below is a simple prototype of building linear and logistic regression with Keras, and tests it on a simulated dataset. . class LinearRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a linear activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;linear&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) class LogisticRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a sigmoid activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) . Understanding Hidden Layers . References Hornik, K., Stinchcombe, M., &amp; White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5), 359–366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 | Nelder, J. A., &amp; Wedderburn, R. W. M. (1972). Generalized Linear Models. Journal of the Royal Statistical Society. Series A (General), 135(3), 370–384. http://www.jstor.org/stable/2344614 | . Footnotes . the nuances of fitting these models (e.g. Maximum Likelihood Estimation, Backfitting, Gradient Descent) are not covered in detail in this blog post. There are links to papers and lectures on these topics in their corresponding sections if you would like to read about them.↩ | The closed form solution for OLS is $ beta = (X^TX)^ inv X^Ty$. This requires $X^TX$ to be invertible, which is the case when the elements in $X$ are linearly independent. This is satisfied by our assumption of independence. Without this assumption, there is no closed form solution, and $ beta$ can be approximated by the maximum likelihood estimation function: $min_ beta(y - beta X)^T(y - beta X)$.↩ | The exponential family is a particular family of probability distributions such that their probability density function (PDF) can be writted as: $P(x | theta) = f(x) g( theta) exp Big( eta( theta) centerdot T(x) Big)$, where $f$, $g$, $ eta$, and $T$ are known functions and $ theta in Reals$ is the only parameter to the PDF.↩ | This explains why we could create a model identical to linear and logistic regression by designing a neural network with zero hidden layers.↩ | The &quot;smoothness&quot; of a function is described by the continuity of the derivatives. The set of functions with a smoothness of 0 is equivalent to the set of continuous functions. The set of functions with a smoothness of 1 is the set of continuous functions such that their first derivative is continuous. So on, and so forth. Generally, a function is considered &quot;smooth&quot; if it has &quot;smoothness&quot; of $ infty$. In other words, it is infinitely differentiable.↩ | The actual function being fit here is $f(x_1,x_2) = a(x_1) + b(x_2)$, however I plot the function $f(x_1 + x_2) = a(x_1) + b(x_2)$ in order to project it as two-dimensional, as that is easier for readers to look at.↩ | Technically, this could be fit where the sub-networks take more than a single feature as input, but this comes at a cost of interpretability. It is still possible to explore the relationship between both features and the output, however it becomes high-dimensional, entangled, and hence more difficult to interpret.↩ |",
            "url": "https://ryansaxe.com/fundamentals/beginner/2021/02/21/LinearNN.html",
            "relUrl": "/fundamentals/beginner/2021/02/21/LinearNN.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Designing Transparent Neural Networks",
            "content": "Most systems we interact with are part of some pipeline that integrates Machine Learning (ML). Sometimes we interact with an ML model directly, like Spotify&#39;s recommender system for songs. Other times, this interaction is more detatched; we post a comment on Twitter or Facebook, and this comment is used to train some language model at the respective company. As ML models become more and more prevalent, interpreting and explaining the decisions these models make become increasingly important. . Neural networks (NNs) are a popular class of machine learning algorithms, which are notorious for being difficult to interpret. They are often referred to as &quot;black boxes&quot; or &quot;opaque&quot;. Linear Models, Generalized Linear Models (GLMs), and Generalized Additive Models (GAMs) are examples of popular machine learning algorithms that are not &quot;opaque&quot;, but instead &quot;transparent&quot;. This transparency often leads linear and additive models to be favorable choices over classic neural networks even though the functions neural networks can learn are a superset of the functions these other models can learn. The goal of this blog post is to demonstrate that, with a particular asterix over the architecture of neural networks, they can be just as transparent as linear models. . . Prior to jumping into such a neural architecture, it&#39;s important to understand the fundamental transparent algorithms, starting from the simplest linear models. The following section will provide a background on the math and fundamentals of linear and additive models, as well as how they relate to the inner mechanisms of neural networks1. . . Note: If you are familiar with GLMs, GAMs, and NNs, feel free to skip this introductory section. . . Introducing Linear and Additive Models . Linear models are one of the simplest approaches to supervised learning. The general goal of supervised learning is to discover some function $f$ that minimizes an error-term $ epsilon$ given a set of input features $X$ and a corresponding target $y$ such that $y = f(X) + epsilon$. Additionally, the output of a supervised model is often written as $ hat{y} = f(X)$ because $f(X)$ is our best approximation of $y$. . Different algorithms are sufficient for learning different functions. Simple linear models can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_n X_n + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$, and $T$ is the transpose operation, which in this case is basically identical to computing the dot product of two n-dimensional vectors. . Linear Regression . Linear regression is arguably the simplest linear model, and comes with four assumptions: . Linearity: The relationship between $X$ and the mean of $y$ is linear. | Independence: $X_i$ and $X_j$ are linearly independent of eachother for all $i neq j$. | Normality: $y$ given any $X$ comes from a normal distribution. | Homoscedasticity: The variance of the error is the same for any value of $X$. | These assumptions can be nicely described by one math equation: . $$ begin{aligned} y &amp; in mathcal{N}(X^T beta, sigma^2 I) &amp; Rightarrow mathbb{E}[y|X] = mu(X) = X^T beta end{aligned} $$Unfortunately, these assumptions are quite rigid for the real world. Many datasets do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can perform the regression task without such rigid assumptions? The common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. | Little Data: Ordinary Least Squares (OLS) is a closed form solution to linear regression2. | Interpretability: $y$ can be explained with respect to how $X$ interacts with the $ beta$ coefficients. | Today, we are going to stick with the notion that transparency is of the utmost importance, and assume we have a significant amount of data. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, these models don&#39;t need to be exactly linear in order to be interpretable. . Generalized Linear Models (GLMs) . Generalized Linear Model (GLM), introduced in (Nelder &amp; Wedderburn, 1972), loosen the constraints of normality, linearity, and homoscedasticity described in the previous section. Furthermore, GLMs break down the problem into three different components: . Random Component: The probability distribution of $y$ (typically belonging to the exponential family3). | Systematic Component: the right side of the equation for predicting $y$ (typically $X^T beta$). | Link Function: A function $g$ that links the systematic component and the random component. | This yields the following general equation for GLMs: . $$g( mathbb{E}[y|X]) = X^T beta + epsilon$$ . Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the corresponding GLM is exactly linear regression! Hence, the functions that GLMs can describe are a superset of the functions linear regression can describe. . Selecting a link function according to the random component is what differentiates GLMs. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(- infty,+ infty)$, as that is the expected range of $X^T beta$. As an example, binary logistic regression assumes the probability distribution of $y$ is a bernoulli distribution. This means that the average of the distribution, $ mu$, is between 0 and 1. We need some function $g: [0,1] rightarrow Reals$, and the logit function is sufficient for this: . $$g( mu) = log( frac{ mu}{1 - mu})$$ . Now, we can fit a simple linear model to $g(y) = X^T beta + epsilon$. Unfortunately, introducing a non-linear transformation to this equation means that Ordinary Least Squares is no longer a reasonable estimation method. Hence, learning $ beta$ requires a different estimation method. Maximum Likelihood Estimation (MLE) estimates the parameters of a probability distribution by maximizing the likelihood that a sample of observed data belongs to that probability distribution. In fact, under the assumptions of simple linear regression, MLE is equivalent to OLS as demonstrated on page 2 of these CMU lecture notes. The specifics of MLE are not necessary for the rest of this blog post, however if you would like to learn more about it, please refer to these Stanford lecture notes. . The Building Blocks of Neural Networks . But where do neural networks come in? Aren&#39;t they incredibly non-linear and opaque, unlike GLMs? Sort of. On the macro-level, NNs and GLMs look very different, but the micro-level tells a different story. Let&#39;s zoom into the inner workings of neural networks and see how they relate to GLMs! . Neural networks are built of components called layers. Layers are built of components called nodes. At their heart, these nodes are computational-message-passing-machines. They receive a set of inputs, perform a computation, and pass the result of that computation to other nodes in the network. These are the building blocks of neural networks. . The first layer of a neural network is called the input layer, because each node passes an input feature to all nodes in the next layer. The last layer of a neural network is called the output layer, and it should represent the output you are trying to predict (this layer has one node in the classic regression case). Lastly, any layers between the input and output layers are called hidden layers. . . In the classic fully-connected feed-forwad neural network, this structure of layers is ordered and connected such that every node $n_j$ in layer $L_i$ receives the output of every node in the predecing layer $L_{i-1}$, does some computation with those outputs, and passes the corresponding output to each node in the succeeding layer $L_{i + 1}$. The image above displays a neural network with $N$ input features, a single hidden layer, and a single output prediction $ hat{y}$. . Each node in layer $L_i$ contains some set of weights ($w$) and a bias ($b$), where the dimension of the weight vector is equal to the number of nodes in layer $L_{i - 1}$. When the node receives the output of all the nodes in the preceding layer, it performs the following computation: $L_{i - 1}^Tw + b$. . This should look familiar! It is quite literally $X^T beta$: the classic computation from linear models on the ouputs of the preceding layer! . However, before this node passes $X^T beta$ to the next layer in the network, it is passed through an activation function $f$. Activation functions often introduce non-linearity to the neural network, similar to link functions in GLMs. The image below isolates a single neuron from the image above, taking input from the previous layer, and making a prediction by transforming the output of the neuron with an activation function $ hat{y} = f(X^T beta)$. . . In fact, observe that if the activation function is invertible, this computation is equivalent to $f^{-1}( hat{y}) = X^T beta$, which is exactly a GLM with link function $f^{-1}$. This demonstrates that the computation of a single node in a neural network is, conceptually, a GLM on the output of the previous layer! . Furthermore, this means that a neural network with zero hidden layers and a linear activation function on the output layer is exactly equivalent to linear regression, as the lack of hidden layers maintains independence. And, if we change the activation function to the inverse of the logit function (this is the sigmoid activation function), this neural network becomes exactly equivalent to logistic regression! The code below is a simple prototype of building linear and logistic regression with Keras, and tests it on a simulated dataset. . class LinearRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a linear activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;linear&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) class LogisticRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a sigmoid activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) . However, It isn&#39;t always as simple as the cases of linear and logistic regression. Not all activation functions are invertible (e.g. ReLU), and hence add non-linearities in ways that are not consistent with GLMs. At the same time, the principal of the link function is to transform $y$ to the proper space, and ReLU does accomplish this under the assumption that $y$ cannot be negative. . There is clearly an intimate connection between neural networks and linear models, as the computational components of neural networks are quite literally non-linear transforms on linear models just like GLMs. So, why are neural networks opaque and GLMs transparent? Let&#39;s look at the math for regression using a neural network with $k$ hidden layers, where $w_{i,j}$ and $b_{i,j}$ are the weights and bias of the jth node in layer $L_i$ with activation function $f_i$. . $$ begin{aligned} L_0&amp; = big [ X_1, X_2, cdots, X_n big ] L_1 &amp; = big [ hspace{0.5em}f_1(L_0^Tw_{1,1} + b_{1,1}), hspace{0.5em}f_1(L_0^Tw_{1,2} + b_{1,2}), cdots big ] &amp; vdots L_k &amp; = big [ hspace{0.5em}f_k(L_{k - 1}^Tw_{k,1} + b_{k,1}), hspace{0.5em}f_k(L_{k - 1}^Tw_{k,2} + b_{k,2}), cdots big ] hat{y} &amp; = L_k^T beta end{aligned} $$Observe that for every layer after $L_1$, the information passed to nodes in that layer from the preceding layer are dependent on every input feature from the input layer. Hence, the inputs to the node are not linearly independent. This means that the restriction of independence breaks once a single hidden layer is introduced to the network4. This dependency is what creates the opacity of neural networks, not the non-linearity. This can be seen in the next section on Generalized Additive Models, a transparent approach with highly non-linear transforms to the input features, while maintaining some notion of independence. This will shed light on how to design neural networks with control over feature dependence! . Generalized Additive Models (GAMs) . Generalized Additive Models (GAMs), introduced in (Hastie &amp; Tibshirani, 1986), take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling: . Allow non-linearity: GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth5. It also can be non-parametric. . | Feature interaction: The systematic component can be an equation that contains non-linear feature interaction like $h_k(X_i,X_j, cdots)$. . | Hence, equations for GAMs look like this: . $$g( mu(X)) = beta_0 + h_1(X_1) + h_2(X_2, X_3) + cdots + h_m(X_n) + epsilon$$ . Technically, this makes GLMs a special case of GAMs where all functions $h_i$ simply multiply their corresponding input feature(s) by a single parameter $ beta_i$. However, unlike GLMs, these functions require a more convoluted fitting mechanism. If you are interested in the history of GAMs and how they are fit, please refer to the original papers on the backfitting algorithm: (missing reference). What follows is a simplification to provide intuition on what these $h_i$ functions are. . $$h_k(x_i) = sum_{j=1}^n b_j(x_i) beta_j$$ . Where $b_j$ is a basis-spline (often called a b-spline), $ beta_j$ is a learned coefficient corresponding to $b_j$, and $n$ is a hyperparameter describing the number of b-splines to use to fit the GAM. A linear combination of b-splines uniquely describes any spline function sharing the same properties (e.g. knots, degrees) as the b-splines (Prautzsch et al., 2002), which means these $h_k$ functions are spline functions. Below is an example of fitting a smooth function ($h_k(x_i) = sin(x_i)$) using a linear combination of cubic b-splines. . As you can see, a linear combination of many cubic b-splines was sufficient to fit this smooth function. GAMs try and fit spline functions to transform each individual dependent variable, and model the independent variable as a linear combination of these spline functions. This maintains transparency because, once fit, we can inspect the learned functions to understand exactly how our model makes predictions according to individual features. . There is so much more to learn about GAMs, such as how these splines are learned, methods of interpreting them, and important regularization penalties to ensure higher degrees of smoothness. There are even some newer methods of fitting GAMs using decision trees instead of splines. If you would like a more extensive review on GAMs, please refer to this wonderful mini-website-textbook, however that&#39;s out of the scope of this blog post. . Now you should finally have the requisite knowledge and background on transparent models that we can get into building transparent neural networks! . Transparent Neural Networks . (Hornik et al., 1989) is the original paper suggesting NNs are a type of universal approximator. The theory of this contribution was explored in multi-layer perceptrons in (Pinkus, 1999) and generally formalized by (Balázs, 2001). The theorem can be summarized by: . A neural network with a single hidden layer of infinite width can approximate any continuous function. . If neural networks can be used to approximate any continuous function, then they can be used to approximate the non-linear, non-parametric, functions ($h_k$ in the previous section) necessary for Generalized Additive Models. Furthermore, neural networks can describe a wider set of functions than GAMs because continuous functions don&#39;t have to be smooth, while smooth functions have to be continuous. . Above are images of three functions. The first function is a linear combination of the second and third function6. Observe that all three functions are continuous. We would like to build a model that can fit the first function, while maintaining feature-wise interpretability such that we can see that it properly learns the second and third functions. Unfortunately, it is not reasonable to expect a GAM to achieve this because, while the functions are continuous, they are not smooth. Below is a GAM with very very low regularization and smoothness penalties in order to let it try and fit non-smooth functions. Furthermore, it is trained and tested on the same dataset. This is a strong demonstration that GAMs cannot approach these problems, because they can&#39;t even overfit to the solution. . from pygam import LinearGAM from pygam import s as spline #fit a classic GAM with no regularization or smoothing penalties to try and #let it overfit to non-smooth functions. It still fails! gam = LinearGAM( spline(0,lam=0,penalties=None) + spline(1,lam=0,penalties=None), callbacks=[] ).fit(X, y) . While these fits aren&#39;t terrible in terms of prediction error, that&#39;s not what we care about. We care about learning the proper structure for the functions that explain the relationship between individual features and our output. The above plots are a clear demonstration that GAMs can&#39;t even overfit to provide that. . Luckily, because these functions are continuous, we can use a neural network to approximate them! . Generalized Additive Neural Networks (GANNs) . The trick is to use a different neural network for each individual feature, and add them together just like how GAMs work! By replacing the non-linear, non-parametric, functions in GAMs by neural networks, we get get Generalized Additive Neural Networks (GANNs), introduced in (Potts, 1999). Unfortunately, this contribution did not take off because we didn&#39;t have the technical capacity to train large networks as we do today. Luckily, now it is quite easy to fit such a model. . . GANNs are simply a linear combination of neural networks, where each network only observes a single input feature7. Because each of these networks take a single input feature, and provide a single output feature, it becomes possible to plot a two-dimensional graph where the x-axis is the input feature and the y-axis is the output feature for each network. This graph is hence a fully transparent function describing how the neural network learned to transform the input feature as it contributes, additively, to the prediction. Hence this type of neural architecture is sufficient for creating a model as transparent as the linear models described in this blog postso far. . Below is code for creating a GANN using Keras. As you can see, this model is capable of solving the regression problem while maintaining feature-wise transparency on piecewise continous functions! . from tensorflow.keras import layers, regularizers #define a simple multi-layer perceptron class NN(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) # relu helps learn more jagged functions if necessary. self.l1 = layers.Dense(8, activation=&#39;relu&#39;) # softplus helps smooth between the jagged areas from above # as softplus is referred to as &quot;SmoothRELU&quot; self.l2 = layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Generalized Additive Neural Network for n features class GANN(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features # initialize MLP for each input feature self.components = [NN() for i in range(n_features)] # create final layer for a linear combination of learned components self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): #split up by individual features individual_features = tf.split(x, self.n_features, axis=1) components = [] #apply the proper MLP to each individual feature for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) #concatenate learned components and return linear combination of them components = tf.concat(components, axis=1) return self.linear_combination(components) . model = GANN(n_features=2) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=50, batch_size=32, ) . At first glance, these results look good, but not perfect. The first plot demonstrates this model achieved a near perfect fit of the actual task, while the last two plots look like the Neural Additive Model was capable of learning the general shapes of the functions, but is off on the intercepts for all of them. This should not discount the validity of the model. The corresponding math demonstrates perfectly fitting intercepts of an additive model cannot be guaranteed. . Let $h_i(X_i) = alpha_i + f(X_i)$ where $f(X_i)$ represents all of the aspects of $h(X_i)$ that dependent on $X_i$, and $ alpha_i$ represents the intercept. . $$ begin{aligned} y &amp; = beta_0 + sum_{i=1}^n beta_i h_i(X_i) &amp; = beta_0 + sum_{i=1}^n beta_i ( alpha_i + f_i(X_i)) &amp; = beta_0 + sum_{i=1}^n beta_i alpha_i + sum_{i=1}^n beta_i f_i(X_i) end{aligned} $$The only way to tease apart these intercepts is via $ beta$. Imagine the proper fit of this equation, for every $i$, had $ beta_i = 1$ and $ alpha_i = 2$. In this case, if half of the learned $ alpha_i$s are zero, and the other half are four, that would yield the exact same result for $ sum_{i=1}^n beta_i alpha_i$ as the proper fit. Hence, by way of contradictory example, it is impossible to guarantee learning correct intercepts for the individual components of any additive model. . The plots below are what happens when we simply adjust the learned intercepts for these functions. As you can see, by simply changing the intercept, we are able to show a near perfect fit of these functions, which is the best we can ever hope to do! Furthermore, we can explore the derivatives of these functions to measure the goodness of fit because the rate of change of the function is entirely independent of the intercept. . This is a clear deomonstration that Generalized Additive Neural Networks are capable of overfitting to piecewise continuous functions while maintaining transparency! . The next section provides a brief overview of Neural Additive Models (NAMs), which are a special variant of GANNs that are designed to be able to fit &quot;jumpy functions&quot;, as this is more reminiscient of real-world data. . Neural Additive Models (NAMs) . Real world data doesn&#39;t look like beautifully continuous functions. It&#39;s often messy, and there will be multiple data points with extremely similar features that have noticeably different results. Many machine learning models have existing structure that prevents learning functions that look crazy and jump all over the place. Linear regression has to learn the best fitting line because $X^T Beta$ can&#39;t describe a &quot;jumpy&quot; function. GAMs are regularized to enforce smoothness to prevent this as well. (Agarwal et al., 2020) explores a particular modification to the mathematic computations made on nodes in GANNs in order to let them fit &quot;jumpy&quot; functions. They call these modified nodes &quot;exp-centered hidden units&quot; or &quot;ExU units&quot;, and they work as follows: . Let $w$ and $b$ be the respective weight and bias parameters on a node with activation function $f$. Then, when the node is passed input $x$ it computes: $f(e^w * (x - b))$. Recall that the normal computation on these nodes is simply applying the activation function to a linear computation: $f(w * x + b)$. The reason exponentiation of the weights accomplishes the goal of learning &quot;jumpy&quot; functions is that small changes in the input can have drastic changes on the output, enabling small weights to still represent a function with a steep slope. The exerpt below is taken directly from (Agarwal et al., 2020) and demonstrates the difference between a non-regularized neural network&#39;s ability to overfit to data when using normal node computation (a) versus ExU unit computation (b). . . While this clearly demonstrates ExU&#39;s ability to overfit to jumpy functions, this isn&#39;t ideal. Overfitting isn&#39;t a good thing. Hence, the rest of this paper demonstrates ExU can be regularized to learn functions that are relatively smooth, yet &quot;jumpy&quot; at very specific junctions. This is more reminiscient of real-world solutions, and the paper shows that this can outperform current state-of-the-art GAMs while maintaining feature transparency. Their regularization strategies are: . Dropout: randomly zero-out nodes inside each NN. | Weight Decay: add a penalty corresponding to the L2 norm of weights inside each NN. | Output Penalty: add a penalty corresponding to the L2 norm of the outputs of each NN. | Feature Dropout: randomly exclude entire features from the NAM. | ExU and these regularization techniques are the only difference between NAMs and GANNs, and it fundamentally changes the loss landscape in a way that enables learning &quot;jumpy&quot; functions. GANNs are a very general architectural description, and there are many perturbations on the architecture, such as NAMs, that can make them well suited for your problem. The final section in this blog post will discuss the computational graph, and how to leverage understanding a specific problem to design a GANN. . Conclusion: a Transparent Computational Graph . Hopefully the review of the NAM paper motivated some curiosity and creativity. The nodes and feature-networks in the image above don&#39;t need to be the classic feed-forward-neural-network you may be familiar with. Furthermore, there is no reason one of these feature-networks have to be the same. They can have different numbers of layers, different activation functions, and different architectures all together. . A neural network can be described as a computational graph. It&#39;s a graph that describes a directed order of computation in order to go from some input to some output. Let me remind you of the general GANN architecture, or computational graph: . . Each feature $x_i$ is processed by some function $f_k$. Furthermore, remember that, while for transparency it&#39;s ideal to have $f_k$ only look at a single feature $x_i$, it&#39;s possible to learn functions dependent on multiple features: . $$ hat{y} = beta_0 + beta_1 f_1(x_1, x_2) + beta_2 f_2(x_3) + beta_3 f_3(x_4, x_5, x_6) + cdots$$ . Your goal should not be to arbitrarily select neural networks for learning these different functions, but rather design your optimization problem to solve an equation where you can interpret the learned aspects of that equation with respect to features. Consider the following example: . Let&#39;s say you have 5 features $[x_1, x_2, x_3, x_4, x_5]$. And let&#39;s say that you believe, according to features $x_1$ and $x_2$, the solution to your problem should treat the other features differently. We can describe our equation as follows: . Let $f_1(x_1, x_2) in {0,1 }$ . Let $f_2(x_3, x_4, x_5) in Reals$ . Let $f_3(x_3, x_4, x_5) in Reals$ . Then, we want to solve: . $$ hat{y} = (f_1(x_1, x_2)) * f_2(x_3, x_4, x_5) + (1 - f_1(x_1, x_2)) * f_3(x_3, x_4, x_5) $$Observe that this function isn&#39;t entirely additive with respect to each function $f$. Specifically, the purpose of $f_1$ is to learn some binary feature to determine how the model handles the other features. if $f_1(x_1, x_2) = 0$, then the model will use $f_3$, otherwise it will use $f_2$. While this isn&#39;t the exact same structure as a GANN, it is still quite interpretable. We can inspect the differences between functions $f_2$ and $f_3$ with respect to how $f_1$ determines which function to use. . This whole blog post built up to understanding of how additive models can maintain feature-wise interpretability, and that it&#39;s possible to design a neural network architecture that leverages that. However, the computational graph generalizes even further, and I hope the example above helped demonstrate that. Focus on the math. Focus on the specifics of the problem at hand. You can design interpretable architectures as a function of these feature-networks as long as you meticulously control how the features interact according to the computational graph. Then, because you controlled and isolated features in the computational graph, you can easily discern how those features were used in order to make the final prediction $ hat{y}$, which is the definition of a transparent model! . References Nelder, J. A., &amp; Wedderburn, R. W. M. (1972). Generalized Linear Models. Journal of the Royal Statistical Society. Series A (General), 135(3), 370–384. http://www.jstor.org/stable/2344614 | Hastie, T., &amp; Tibshirani, R. (1986). Generalized Additive Models. Statist. Sci., 1(3), 297–310. https://doi.org/10.1214/ss/1177013604 | Prautzsch, H., Boehm, W., &amp; Paluszny, M. (2002). Bézier and B-Spline Techniques. https://doi.org/10.1007/978-3-662-04919-8 | Hornik, K., Stinchcombe, M., &amp; White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5), 359–366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 | Pinkus, A. (1999). Approximation theory of the MLP model in neural networks. Acta Numerica, 8, 143–195. https://doi.org/10.1017/S0962492900002919 | Balázs, C. (2001). Approximation with Artificial Neural Networks. https://doi.org/10.1.1.101.2647 | Potts, W. J. E. (1999). Generalized Additive Neural Networks. Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 194–200. https://doi.org/10.1145/312129.312228 | Agarwal, R., Frosst, N., Zhang, X., Caruana, R., &amp; Hinton, G. E. (2020). Neural Additive Models: Interpretable Machine Learning with Neural Nets. | . Footnotes . the nuances of fitting these models (e.g. Maximum Likelihood Estimation, Backfitting, Gradient Descent) are not covered in detail in this blog post. There are links to papers and lectures on these topics in their corresponding sections if you would like to read about them.↩ | The closed form solution for OLS is $ beta = (X^TX)^ inv X^Ty$. This requires $X^TX$ to be invertible, which is the case when the elements in $X$ are linearly independent. This is satisfied by our assumption of independence. Without this assumption, there is no closed form solution, and $ beta$ can be approximated by the maximum likelihood estimation function: $min_ beta(y - beta X)^T(y - beta X)$.↩ | The exponential family is a particular family of probability distributions such that their probability density function (PDF) can be writted as: $P(x | theta) = f(x) g( theta) exp Big( eta( theta) centerdot T(x) Big)$, where $f$, $g$, $ eta$, and $T$ are known functions and $ theta in Reals$ is the only parameter to the PDF.↩ | This explains why we could create a model identical to linear and logistic regression by designing a neural network with zero hidden layers.↩ | The &quot;smoothness&quot; of a function is described by the continuity of the derivatives. The set of functions with a smoothness of 0 is equivalent to the set of continuous functions. The set of functions with a smoothness of 1 is the set of continuous functions such that their first derivative is continuous. So on, and so forth. Generally, a function is considered &quot;smooth&quot; if it has &quot;smoothness&quot; of $ infty$. In other words, it is infinitely differentiable.↩ | The actual function being fit here is $f(x_1,x_2) = a(x_1) + b(x_2)$, however I plot the function $f(x_1 + x_2) = a(x_1) + b(x_2)$ in order to project it as two-dimensional, as that is easier for readers to look at.↩ | Technically, this could be fit where the sub-networks take more than a single feature as input, but this comes at a cost of interpretability. It is still possible to explore the relationship between both features and the output, however it becomes high-dimensional, entangled, and hence more difficult to interpret.↩ |",
            "url": "https://ryansaxe.com/transparency/2021/02/17/NAM.html",
            "relUrl": "/transparency/2021/02/17/NAM.html",
            "date": " • Feb 17, 2021"
        }
        
    
  

  
  

  

  

  

  

  
  

  

  
  

  

  

  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}