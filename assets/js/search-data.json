{
  
    
        "post0": {
            "title": "Designing Interpretable Neural Networks",
            "content": "Any supervised prediction problem can be described as learning some function $f$ that minimizes the error to the equation $f(x) = y$, where $x$ is the input features and $y$ is the target prediction. This blog post will explore methods to devise neural architectures to learn such functions, with the additional goal of making them as transparent as linear models. . Prior to delving straight into the neural networks, we should work our way up the hierarchy of linear models. Starting from basic linear models, to generalizing them with the aptly named General Linear Model (GLM), and eventually removing linearity with Generalized Additive Models (GLMs) and beyond! . Linear Models . Different algorithms are sufficient for learning different families of functions. For example, simple linear models like linear regression can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_N X_N + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$, and $ epsilon$ is the error-term (often referred to as the residual). Furthermore, linear regression comes with some additional assumptions: . Linearity: The relationship between $X$ and the mean of $Y$ . The namesake of linear models: linearity. This is a rigid assumption. Linear regression cannot properly solve a problem where this does not hold. In fact, a lack of this relationship will often be reflected in the error terms in a way that breaks the other assumptions. . | Independence: Observations are independent of each other. . Linear regression assumes that there are no relationships between the features. For example, a model that has a feature for price and a feature for discount would break this assumption. As discount changes, the price changes accordingly, creating a dependence between these features. This type of dependence is referred to as multicolinearity. . | Normality: We assume that $y$ given $X$ is a gaussian with mean dependent only on $X$ and covariance $ sigma^2 I$ . Placeholder explanation . | Homoscedasticity: The variance of residual is the same for any value of $X$ . Homoscedasticity is a very nice property. Regardless of the input features, the error in your model stays the same. It has no variance. Heteroscedasticity often implies a lack of linearity, independence, or normality. It&#39;s a hint that there are complexities in the data that cannot be represented by your current model. If your linear model has variance within the error terms, this implies that you should probably explore other methodologies, but not that your model is objectively wrong. . | However, these assumptions are quite rigid. So why do we still use linear regression when we have algorithms that can comparably perform the regression task without such rigid assumptions? The two most common answers to this question. . Occam&#39;s Razor. Linear regression is the simplest way of solving a regression problem, and it&#39;s often a good idea to start simple. | Interpretable with respect to the input features. Since linear regression fits an exact equation known apriori, when the model spits out $ beta_2$, that means the feature $x_2$ contributes $ beta_2 x_2$ to the prediction. | In many scenarios, the learned $ beta$ coefficients are more valuable than the actual prediction. Let&#39;s say you (unadvisably so) devised a linear model for predicting the expected salary of an individual, and that model (also unadvisably so) contained sensitive factors like race or gender. If the model learns high coefficients with respect to those features, well, you have a sexist and/or racist model! While you probably shouldn&#39;t have even attempted building this model, at least it was fully transparent so you know it is unfair and hence won&#39;t deploy it (I hope). . Today, we are going to stick with the notion that transparency is of the utmost importance. That whatever the model is, it must be able to produce feature-wise explanations that are useful. However, contrary to what you may have heard, these models don&#39;t need to be exactly linear as described above. . Generalized Linear Models . Given the assumption of normality, $ mu(x) = mathbb{E}[y|X]$ was always the mean from the normal distribution. Generalized Linear Models (GLMs) loosen the constraints of normality. Now, $ mu$ can represent an expectation that can come from any distribution in the exponential family, which just means that an exponential shows up in the probility density function (pdf). This includes quite a lot of extra distributions, such as gamma, bernoulli, poisson, and more! . However, not all of these distributions range from $- infty$ to $+ infty$, which is necessary for solving regression problems in the linear form $y = X^T beta$. In order for us to maintain a linear model, we transform the mean of the distribution ( $ mu(x) = mathbb{E}[y|X]$ ) using a specified function $g$. $g$ is called the link function. . $$g( mu(x)) = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_N X_N + epsilon$$ . Relating to Neural Networks . Generalized Additive Models . GAMs take another step towards reducing the restrictions within linear models. There is exactly one modification on GLMs that turn them into GAMs: . $$y = beta_0 + beta_1 g(X_1) + beta_2 g(X_2) + cdots + beta_N g(X_N)$$ . where $g$ is allowed to be a non-linear function. This eliminates the restriction of linearity. Furthermore, GAMs are allowed to model dependent features . gam = LinearGAM(spline(0) + spline(1) + spline(2)).fit(X, y) . Neural Additive Models . # relu helps learn more jagged functions if necessary class MLP(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) self.l1 = tf.keras.layers.Dense(8, activation=&#39;relu&#39;) self.l2 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = tf.keras.layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Neural Additive Model for n features class NAM(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features self.components = [MLP() for i in range(n_features)] self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): individual_features = tf.split(x, self.n_features, axis=1) components = [] for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) components = tf.concat(components, axis=1) return self.linear_combination(components) . Train on 1000000 samples Epoch 1/10 1000000/1000000 [==============================] - 60s 60us/sample - loss: 414.4350 - mean_absolute_error: 10.9988 Epoch 2/10 1000000/1000000 [==============================] - 57s 57us/sample - loss: 120.1113 - mean_absolute_error: 4.9874 Epoch 3/10 1000000/1000000 [==============================] - 55s 55us/sample - loss: 74.6649 - mean_absolute_error: 3.6563 Epoch 4/10 1000000/1000000 [==============================] - 54s 54us/sample - loss: 59.5442 - mean_absolute_error: 3.0831 Epoch 5/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 51.1148 - mean_absolute_error: 2.6067 Epoch 6/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 46.4825 - mean_absolute_error: 2.3151 Epoch 7/10 1000000/1000000 [==============================] - 55s 55us/sample - loss: 43.2321 - mean_absolute_error: 2.1660 Epoch 8/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 41.0939 - mean_absolute_error: 2.0531 Epoch 9/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 39.4331 - mean_absolute_error: 1.9772 Epoch 10/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 37.6433 - mean_absolute_error: 1.9064 . &lt;tensorflow.python.keras.callbacks.History at 0x21904d6d948&gt; .",
            "url": "https://ryansaxe.com/transparency/2020/12/01/NAM.html",
            "relUrl": "/transparency/2020/12/01/NAM.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  

  

  

  
  

  

  
  

  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}