{
  
    
        "post0": {
            "title": "Designing Interpretable Neural Networks",
            "content": "Most supervised prediction problems can be described as learning some function $f$ that minimizes the error-term ($ epsilon$) to the equation $y = f(X) + epsilon$, where $X$ is the input features and $y$ is the target prediction. This blog post will explore methods to devise neural architectures to learn such functions, with the additional goal of making them as transparent as linear models. However, prior to jumping into such architectures, it&#39;s important to understand the foundation of classic transparent approaches, starting from the simplest linear models. . The next section provides the necessary background to understand how the building blocks of neural networks are related to linear models. Then, we can drop the restriction of linearity with Generalized Additive Models (GAMs), and explore building transparent neural networks. . Linear Models . Different algorithms are sufficient for learning different families of functions. For example, simple linear models can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_n X_n + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$. . Linear Regression . Linear regression is arguably the simplest of the linear models, and comes with four assumptions: . Linearity: The relationship between $X$ and the mean of $Y$ is linear. | Independence: $X_i$ and $X_j$ are independent of eachother for all $i neq j$. | Normality: $y$ given any $X$ comes from a normal distribution. | Homoscedasticity: The variance of residual is the same for any value of $X$. | These assumptions can be nicely described by one math equation: . $$ begin{align} y &amp; in mathcal{N}(X^T beta, sigma^2 I) &amp; Rightarrow mathbb{E}[y|X] = X^T beta end{align} $$However, these assumptions are quite rigid for the real world. Many datasets and/or problem spaces do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can comparably perform the regression task without such rigid assumptions? The common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. | Little Data: Ordinary Least Squares (OLS) is an estimation method for linear regression with a closed form solution1. | Interpretability: The prediction $y$ can always be explained with respect to the input features $X$ connected to the learned coefficients $ beta$. | Today, we are going to stick with the notion that transparency is of the utmost importance, and assume we have a significant amount of data. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, contrary to what you may have heard, these models don&#39;t need to be exactly linear as described above in order to be comparably interpretable. . Generalized Linear Models . There are three components to any Generalized Linear Model (GLM): . Random Component: The probability distribution of $y$ (typically belonging to the exponential family2). | Systematic Component: the system of explanatory variables for predicting the random component (often $X^T beta$). | Link Function: A function $g$ that links systematic component and the random component. | This yields the following general equation for GLMs: . $$g( mathbb{E}[y|X]) = X^T beta + epsilon$$ . Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the GLM is exactly linear regression! Hence the functions that GLMs can describe are simply a superset of the functions linear regression can describe. Furthermore, recognize that the random component loosens the constraint of normality, and that the link function alters the constraint of linearity. The relationship between $X$ and the mean of $y$ can be non-linear as long as the relationship between $X$ and the mean of $g(y)$ is linear. Lastly, the residuals are allowed to be heteroscedastic. The only restriction from linear regression that fully remains is the independence of the explanatory variables. . The Link Function . Non-normal random components necessitate a link function. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(- infty,+ infty)$. . However, not all of these distributions range from $- infty$ to $+ infty$, which is necessary for solving regression problems in the linear form $y = X^T beta$. In order for us to maintain a linear model, we transform the mean of the distribution ( $ mu(x) = mathbb{E}[y|X]$ ) using a specified function $g$. $g$ is called the link function. . $$g( mu(x)) = X^T beta + epsilon$$ . Observe that this also loosens the constraints of linearity. Recall that $ mu(x)$ is just our expectation for $y$ given $X$. So, if we substitute back $y$, and take the inverse to get rid of $g$ in relationship to $y$, we get: . $$y = g^{-1}(X^T beta) + epsilon$$ . Hence the relationship between $y$ and $X$ is no longer required to be exactly linear. However, the relationship between the resulting $g(y)$ transformed by the link function and $X$ must be linear. . Note that this clarification implies that L $g$ is the identity function $g( mu) = mu$, then the GLM is just normal linear regression. . Maximum Likelihood Estimation . Note: OLS = MLE under normality constraint . Example: Logistic Regression . Logistic regression is actually a GLM! Let&#39;s say we&#39;re trying to predict some binary variable, which means that $y$ will be either 0 or 1. This can be described by a bernoulli distribution. In order to squash the regression results to be between 0 and 1, we use the logit link function $g( mu) = log( frac{ mu}{1 - mu})$. . And GLMs are as easy as that! Given the expected distribution of your problem, you determine a link function that will properly transform your data to the right space. And then you can fit a simple linear model: $y = X^T beta$. . Relating to Neural Networks . Note: MLE -&gt; crossentropy . . Link Function -&gt; Activation Function . Maximum Likelihood Estimation -&gt; Loss Function . Code Example . class Regression(tf.keras.Model): def __init__( self, name=None, style=&#39;linear&#39; ): super().__init__(name=name) # linear regression uses the identity as the link function # and the inverse of the identity is a linear activation if style.lower() == &#39;linear&#39;: activation = &#39;linear&#39; # logistic regression uses the logit link function, and the # inverse of logit is a sigmoid activation function elif style.lower() == &#39;logistic&#39;: activation = &#39;sigmoid&#39; # no other options are supported else: raise ValueError(&#39;input style only supports two options: linear or logistic&#39;) # pass input directly to the output layer no hidden layers self.output_layer = tf.keras.layers.Dense(1, activation=activation) def call(self, x, training=None): return self.output_layer(x) . logistic_reg = Regression(style=&#39;logistic&#39;) logistic_reg.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = logistic_reg.fit( X.to_numpy(), y_logistic.to_numpy(), epochs=20, batch_size=32, ) . Train on 100000 samples Epoch 1/20 100000/100000 [==============================] - 4s 37us/sample - loss: 0.7754 - mean_absolute_error: 0.4888 Epoch 2/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.6278 - mean_absolute_error: 0.4604 Epoch 3/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.5729 - mean_absolute_error: 0.4327 Epoch 4/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.5302 - mean_absolute_error: 0.4069 Epoch 5/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.4937 - mean_absolute_error: 0.3836 Epoch 6/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.4624 - mean_absolute_error: 0.3629 Epoch 7/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.4352 - mean_absolute_error: 0.3443 Epoch 8/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.4114 - mean_absolute_error: 0.3277 Epoch 9/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.3901 - mean_absolute_error: 0.3125 Epoch 10/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.3716 - mean_absolute_error: 0.2991 Epoch 11/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.3551 - mean_absolute_error: 0.2869 Epoch 12/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.3405 - mean_absolute_error: 0.2760 Epoch 13/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.3273 - mean_absolute_error: 0.2661 Epoch 14/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.3153 - mean_absolute_error: 0.2570 Epoch 15/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.3045 - mean_absolute_error: 0.2487 Epoch 16/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.2947 - mean_absolute_error: 0.2412 Epoch 17/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.2858 - mean_absolute_error: 0.2343 Epoch 18/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.2774 - mean_absolute_error: 0.2277 Epoch 19/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.2698 - mean_absolute_error: 0.2218 Epoch 20/20 100000/100000 [==============================] - 3s 34us/sample - loss: 0.2626 - mean_absolute_error: 0.2162 . binary crossentropy loss: 0.26264412118434904 mean absolute error: 0.21618827 . linear_reg = Regression(style=&#39;linear&#39;) linear_reg.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = linear_reg.fit( X.to_numpy(), y_linear.to_numpy(), epochs=20, batch_size=32, ) . Train on 100000 samples Epoch 1/20 100000/100000 [==============================] - 4s 35us/sample - loss: 16.7596 - mean_absolute_error: 2.5893 Epoch 2/20 100000/100000 [==============================] - 3s 32us/sample - loss: 0.5011 - mean_absolute_error: 0.5516 Epoch 3/20 100000/100000 [==============================] - 3s 33us/sample - loss: 0.0413 - mean_absolute_error: 0.1400 Epoch 4/20 100000/100000 [==============================] - 3s 33us/sample - loss: 0.0023 - mean_absolute_error: 0.0365 Epoch 5/20 100000/100000 [==============================] - 3s 33us/sample - loss: 9.4708e-05 - mean_absolute_error: 0.0057 Epoch 6/20 100000/100000 [==============================] - 3s 33us/sample - loss: 1.6401e-09 - mean_absolute_error: 1.3697e-05 Epoch 7/20 100000/100000 [==============================] - 3s 32us/sample - loss: 1.4325e-07 - mean_absolute_error: 1.0512e-04 Epoch 8/20 100000/100000 [==============================] - 3s 31us/sample - loss: 1.8288e-07 - mean_absolute_error: 1.8046e-04 Epoch 9/20 100000/100000 [==============================] - 3s 31us/sample - loss: 1.9753e-07 - mean_absolute_error: 2.3565e-04 Epoch 10/20 100000/100000 [==============================] - 3s 31us/sample - loss: 2.0450e-07 - mean_absolute_error: 2.3890e-04 Epoch 11/20 100000/100000 [==============================] - 3s 31us/sample - loss: 2.5764e-07 - mean_absolute_error: 1.9322e-04 Epoch 12/20 100000/100000 [==============================] - 3s 31us/sample - loss: 1.7682e-07 - mean_absolute_error: 1.4774e-04 Epoch 13/20 100000/100000 [==============================] - 3s 31us/sample - loss: 2.1005e-07 - mean_absolute_error: 2.1238e-04 Epoch 14/20 100000/100000 [==============================] - 3s 31us/sample - loss: 2.0354e-07 - mean_absolute_error: 1.9896e-04 Epoch 15/20 100000/100000 [==============================] - 3s 31us/sample - loss: 2.2435e-07 - mean_absolute_error: 2.0558e-04 Epoch 16/20 100000/100000 [==============================] - 3s 31us/sample - loss: 1.8651e-07 - mean_absolute_error: 1.4816e-04 Epoch 17/20 100000/100000 [==============================] - 3s 31us/sample - loss: 2.0956e-07 - mean_absolute_error: 2.1453e-04 Epoch 18/20 100000/100000 [==============================] - 3s 31us/sample - loss: 1.9873e-07 - mean_absolute_error: 2.5622e-04 Epoch 19/20 100000/100000 [==============================] - 3s 31us/sample - loss: 2.0934e-07 - mean_absolute_error: 1.9631e-04 Epoch 20/20 100000/100000 [==============================] - 3s 31us/sample - loss: 1.9702e-07 - mean_absolute_error: 1.9863e-04 . mean squared error: 1.9702294863463975e-07 mean absolute error: 0.00019862552 . Generalized Additive Models . Generalized Additive Models (GAMs) take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling: . Full elimination of the linearity restriction. . GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth (differentiable everywhere). It also can be non-parametric. . | A change to the independence criterion. . Linear models make the assumption that $X_i$ and $X_j$ are independent forall $i neq j$. Additive models don&#39;t have this property, however we assume that which features interact are known apriori. This means that the systematic component can be an equation that contains non-linear feature interaction like $h_k(X_i,X_j, cdots)$. . | Hence, equations for GAMs look like this: . $$g( mu(X)) = beta_0 + beta_1 h_1(X_1) + beta_2 h_2(X_2, X_3) + cdots + beta_m h_m(X_n) + epsilon$$ . Technically, this makes GLMs a special case of GAMs where all functions $h$ . Fitting Smooth Functions . from pygam import LinearGAM from pygam import s as spline #fit to 20 splines of degree 3, with a low smoothing penalty # and no further constraints to each of the three features gam = LinearGAM(spline(0) + spline(1) + spline(2)).fit(X, y) . mean squared error: 411.93893 mean absolute error: 13.017934 . Additive Neural Networks . Example Code . from tensorflow.keras import layers, regularizers #define a simple multi-layer perceptron class MLP(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) # relu helps learn more jagged functions if necessary. self.l1 = layers.Dense(8, activation=&#39;relu&#39;) # softplus helps smooth between the jagged areas from above # as softplus is referred to as &quot;SmoothRELU&quot; self.l2 = layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Neural Additive Model for n features class NAM(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features # initialize MLP for each input feature self.components = [MLP() for i in range(n_features)] # create final layer for a linear combination of learned components self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): #split up by individual features individual_features = tf.split(x, self.n_features, axis=1) components = [] #apply the proper MLP to each individual feature for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) #concatenate learned components and return linear combination of them components = tf.concat(components, axis=1) return self.linear_combination(components) . model = NAM(n_features=3) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=30, batch_size=32, ) . mean squared error: 33.171770990338324 mean absolute error: 2.0058768 . At first glance, these results look good, but not perfect. While the Neural Additive Model was capable of learning the general shapes of the functions, it looks like it&#39;s off on the intercepts for all of them. This should not discount the validity of the model. The corresponding math demonstrates perfectly fitting intercepts of an additive model cannot be guaranteed. Our NAM is fitting the following function for n features: . $$y = beta_0 + sum_{i=1}^n beta_i h_i(X_i)$$ . Now, $h_i$ is some non-linear function learned via a multi-layer perceptron. Given that any function can have an intercept, we can say that $h_i(X_i) = alpha_i + f(x_i)$ where $f(x_i)$ represents all of the aspects of $h(x_i)$ dependent on $x_i$. . $$ y &amp; = beta_0 + sum_{i=1}^n beta_i ( alpha_i + f_(X_i)) y &amp; = beta_0 + sum_{i=1}^n beta_i alpha_i + sum_{i=1}^n beta_i f_i(X_i) $$The only way to tease apart these intercepts is via $ beta$. Imagine the proper fit of this equation had $ beta_i = 1 forall i$ and $ alpha_i = 2 forall i$. In this case, the first half of $ alpha$ could be zero, and the second half could be four. That would yield the exact same result for $ sum_{i=1}^n beta_i alpha_i$ as the proper fit. Hence it is impossible to guarantee learning correct intercepts for any additive model. . Furthermore, the &quot;goodness of fit&quot; of this model to the true functions can be shown by comparing the partial derivatives. This is because, when taking a derivative with respect to $X$, any aspects of the equation that don&#39;t depend on $X$ (e.g. the intercepts) become zero. . Interpretable Neural Networks . This is the fun part. This is where all your creativity for problem solving given domain knowledge yields an extremely flexible, powerful, and interpretable model! . Priors . Penalties . 1. The closed form solution for OLS is ↩ . 2. The exponential family is a particular family of probability distributions containing an exponential in their probability density function. For further light reading on the topic, please refer to the Wiki.↩ .",
            "url": "https://ryansaxe.com/transparency/2020/12/01/NAM.html",
            "relUrl": "/transparency/2020/12/01/NAM.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  

  

  

  
  

  

  
  

  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}