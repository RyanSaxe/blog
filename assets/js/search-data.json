{
  
    
        "post0": {
            "title": "Designing Transparent Neural Networks",
            "content": "Most systems we interact with are part of some pipeline that integrates Machine Learning (ML). Sometimes we interact with an ML model directly, like Spotify&#39;s recommender system for songs. Other times, this interaction is more detatched; we post a comment on Twitter or Facebook, and this comment is used to train some language model at the respective company. As ML models become more and more prevalent, interpreting and explaining the decisions these models make become increasingly important. . Neural networks are a popular class of machine learning algorithms, which are notorious for being difficult to interpret. They are often referred to as &quot;black boxes&quot; or &quot;opaque&quot;. This blog post will explore a particular type of neural architecture that is inherrantly transparent. However, prior to jumping into that, it&#39;s important to understand the fundamental transparent algorithms, starting from the simplest linear models. . . Note: If you are familiar with NNs, GLMs, and GAMs, feel free to skip this introductory section. . Introducing Linear and Additive Models . Linear models are one of the simplest approaches to supervised learning. The general goal of supervised learning is to discover some function $f$ that minimizes an error-term $ epsilon$ given a set of input features $X$ and a corresponding target $y$ such that $y = f(X) + epsilon$. Additionally, the output of a supervised model is often written as $ hat{y} = f(X)$ because $f(X)$ is our best approximation of $y$. . Different algorithms are sufficient for learning different functions. Simple linear models can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_n X_n + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$, and $T$ is the transpose operation, which in this case is basically identical to computing the dot product of two n-dimensional vectors. . Linear Regression . Linear regression is arguably the simplest linear model, and comes with four assumptions: . Linearity: The relationship between $X$ and the mean of $y$ is linear. | Independence: $X_i$ and $X_j$ are linearly independent of eachother for all $i neq j$. | Normality: $y$ given any $X$ comes from a normal distribution. | Homoscedasticity: The variance of the error is the same for any value of $X$. | These assumptions can be nicely described by one math equation: . $$ begin{aligned} y &amp; in mathcal{N}(X^T beta, sigma^2 I) &amp; Rightarrow mathbb{E}[y|X] = mu(X) = X^T beta end{aligned} $$Unfortunately, these assumptions are quite rigid for the real world. Many datasets do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can perform the regression task without such rigid assumptions? The common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. | Little Data: Ordinary Least Squares (OLS) is a closed form solution to linear regression1. | Interpretability: $y$ can be explained with respect to how $X$ interacts with the $ beta$ coefficients. | Today, we are going to stick with the notion that transparency is of the utmost importance, and assume we have a significant amount of data. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, these models don&#39;t need to be exactly linear in order to be interpretable. . Generalized Linear Models (GLMs) . Generalized Linear Model (GLM), introduced in (missing reference), loosen the constraints of normality, linearity, and homoscedasticity described in the previous section via the following specifications: . Random Component: The probability distribution of $y$ (typically belonging to the exponential family2). | Systematic Component: the right side of the equation for predicting $y$ (typically $X^T beta$). | Link Function: A function $g$ that links the systematic component and the random component. | This yields the following general equation for GLMs: . $$g( mathbb{E}[y|X]) = X^T beta + epsilon$$ . Observe that if the random component is a normal distribution with a constant variance, and the link function is the identity function ($g(y) = y$), then the corresponding GLM is exactly linear regression! Hence, the functions that GLMs can describe are a superset of the functions linear regression can describe. . Selecting a link function according to the random component is what differentiates GLMs. The intuition behind a link function is that it transforms the distribution of $y$ to the range $(- infty,+ infty)$, as that is the expected range of $X^T beta$. As an example, binary logistic regression assumes the probability distribution of $y$ is a bernoulli distribution. This means that the average of the distribution, $ mu$, is between 0 and 1. We need some function $g: [0,1] rightarrow Reals$, and the logit function is sufficient for this: . $$g( mu) = log( frac{ mu}{1 - mu})$$ . Now, we can fit a simple linear model to $g(y) = X^T beta + epsilon$. Unfortunately, introducing a non-linear transformation to this equation means that Ordinary Least Squares no longer guarantees a closed form solution. Hence, learning $ beta$ requires a different estimation method. Maximum Likelihood Estimation (MLE) estimates the parameters of a probability distribution by maximizing the likelihood that a sample of observed data belongs to that probability distribution. In fact, under the assumptions of simple linear regression, MLE is equivalent to OLS as demonstrated on page 2 of these CMU lecture notes. The specifics of MLE are not necessary for the rest of this blog post, however if you would like to learn more about it, please refer to these Stanford lecture notes. . The Building Blocks of Neural Networks . Neural networks are built of components called layers. Layers are built of components called nodes. At their heart, these nodes are computational-message-passing-machines. They receive a set of inputs, perform a computation, and pass the result of that computation to other nodes in the network. These are the building blocks of neural networks. . The first layer of a neural network is called the input layer, because each node passes an input feature to all nodes in the next layer. The last layer of a neural network is called the output layer, and it should represent the output you are trying to predict (this layer has one node in the classic regression case). Lastly, any layers between the input and output layers are called hidden layers. . In the classic fully-connected feed-forwad neural network, this structure of layers is ordered and connected such that every node $n_j$ in layer $L_i$ receives the output of every node in the predecing layer $L_{i-1}$, does some computation with those outputs, and passes the corresponding output to each node in the succeeding layer $L_{i + 1}$. The image below displays a neural network with $N$ input features, a single hidden layer, and a single output prediction $ hat{y}$. . . Each node in layer $L_i$ contains some set of weights ($w$) and a bias ($b$), where the dimension of the weight vector is equal to the number of nodes in layer $L_{i - 1}$. When the node receives the output of all the nodes in the preceding layer, it performs the following computation: $L_{i - 1}^Tw + b$. . This should look familiar! It is quite literally $X^T beta$: the classic computation from linear models on the ouputs of the preceding layer! . However, before this node passes $X^T beta$ to the next layer in the network, it is passed through an activation function $f$. Activation functions often introduce non-linearity to the neural network, similar to link functions in GLMs. The image below isolates a single neuron from the image above, taking input from the previous layer, and making a prediction by transforming the output of the neuron with an activation function $ hat{y} = f(X^T beta)$. . . In fact, observe that if the activation function is invertible, this computation is equivalent to $f^{-1}( hat{y}) = X^T beta$, which is exactly a GLM with link function $f^{-1}$. This demonstrates that the computation of a single node in a neural network is conceptually a GLM on the output of the previous layer! . Furthermore, this means that a neural network with zero hidden layers and a linear activation function on the output layer is exactly equivalent to linear regression. And, if we change the activation function to the inverse of the logit function (this is the sigmoid activation function), this neural network becomes exactly equivalent to logistic regression! The code below is a simple prototype of building linear and logistic regression with Keras, and tests it on a simulated dataset. . class LinearRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a linear activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;linear&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) class LogisticRegression(tf.keras.Model): def __init__(self): super().__init__() # zero hidden layers with a sigmoid activation on one output node self.output_layer = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) def call(self, input_layer, training=None): return self.output_layer(input_layer) . However, It isn&#39;t always as simple as the cases of linear and logistic regression. Not all activation functions are invertible (e.g. ReLU), and hence add non-linearities in ways that are not consistent with GLMs. At the same time, the principal of the link function is to transform $y$ to the proper space, and ReLU does accomplish this under the assumption that $y$ cannot be negative. . There is clearly an intimate connection between neural networks and linear models, as the computational components of neural networks are quite literally non-linear transforms on linear models just like GLMs. So, why are neural networks opaque and GLMs transparent? Let&#39;s look at the math for regression using an arbitrarily defined neural network with $k$ hidden layers. . $$ begin{aligned} L_0&amp; = big [ X_1, X_2, cdots, X_n big ] L_1 &amp; = big [ hspace{0.5em}f_1(L_0^Tw_{1,1} + b_{1,1}), hspace{0.5em}f_1(L_0^Tw_{1,2} + b_{1,2}), cdots big ] &amp; vdots L_k &amp; = big [ hspace{0.5em}f_k(L_{k - 1}^Tw_{k,1} + b_{k,1}), hspace{0.5em}f_k(L_{k - 1}^Tw_{k,2} + b_{k,2}), cdots big ] hat{y} &amp; = L_k^T beta end{aligned} $$Where $w_{i,j}$ and $b_{i,j}$ are the weights and bias of the jth node in layer $L_i$ with activation function $f_i$. . Observe that for every layer after $L_1$, the information passed to nodes in that layer from the preceding layer are dependent on every input feature from the input layer. Hence, the inputs to the node are not linearly independent. This means that the restriction of independence breaks once a single hidden layer is introduced to the network3. This dependency is what creates the opacity of neural networks, not the non-linearity. This can be seen in the next section on Generalized Additive Models, a transparent approach with highly non-linear transforms to the input features, while maintaining some notion of independence. This will shed light on how to design neural networks with control over feature dependence! . Generalized Additive Models (GAMs) . Generalized Additive Models (GAMs), introduced in (Hastie &amp; Tibshirani, 1986), take another step towards reducing the restrictions within linear models. There are two modifications that GAMs make to classic GLMs, which truly moves from rigid assumptions to flexible modeling: . Allow non-linearity: GAMs wrap each component $X_i$ with a function $h_k$, where $h_k$ is some learned function that can be non-linear, but must be smooth4. It also can be non-parametric. . | Allow dependence: Linear models make the assumption that $X_i$ and $X_j$ are linearly independent forall $i neq j$. Additive models don&#39;t have this property, however we assume that which features interact are known apriori. This means that the systematic component can be an equation that contains non-linear feature interaction like $h_k(X_i,X_j, cdots)$. . | Hence, equations for GAMs look like this: . $$g( mu(X)) = beta_0 + beta_1 h_1(X_1) + beta_2 h_2(X_2, X_3) + cdots + beta_m h_m(X_n) + epsilon$$ . Technically, this makes GLMs a special case of GAMs where all functions $h$ are the identity function. . Interpretable Neural Networks . (Hornik et al., 1989) is the original paper suggesting NNs are a type of universal approximator. The theory of this contribution was explored in multi-layer perceptrons in (Pinkus, 1999) and generally formalized by (Balázs, 2001). The theorem can be summarized by: . A neural network with a single hidden layer of infinite width can approximate any continuous function. . This theorem means that neural networks can be used to approximate the feature-wise non-linear, non-parametric, functions ($h_k$ in the previous section) necessary for Generalized Additive Models. Furthermore, neural networks fit via backpropogation can describe a wider set of functions than GAMs fit via backfitting and regularization. This is because continuous functions don&#39;t have to be smooth, while smooth functions have to be continuous. . Above are images of three functions. The middle one is smooth, the other two are piecewise continuous, but not smooth. Hence it is not reasonable to expect a GAM to fit the non-smooth function. Below is a GAM with as little regularization and smoothness penalties as possible. Furthermore, it is trained and tested on the same dataset. This is a strong demonstration that GAMs cannot approach these problems, because they can&#39;t even overfit to the solution. . from pygam import LinearGAM from pygam import s as spline #fit a classic GAM with no regularization or smoothing penalties to try and #let it overfit to non-smooth functions. It still fails! gam = LinearGAM( spline(0,lam=0,penalties=None) + spline(1,lam=0,penalties=None) + spline(2,lam=0,penalties=None), callbacks=[] ).fit(X, y) . While these functions can&#39;t be fit via a GAM, they can be fit via a neural network because they are continuous! But how can we design a neural network that can learn these functions while maintaining transparency? . Generalized Additive Neural Networks (GANNs) . The trick is to use a different neural network for each individual feature, and add them together just like how GAMs work! By replacing the non-linear, non-parametric, functions in GAMs by neural networks, we get get Generalized Additive Neural Networks (GANNs), introduced in (Potts, 1999). Unfortunately, this contribution did not take off because we didn&#39;t have the technical capacity to train large networks as we do today. Luckily, now it is quite easy to fit such a model. . . GANNs are simply a linear combination of neural networks, where each network only observes a single input feature5. Because each of these networks take a single input feature, and provide a single output feature, it becomes possible to plot a two-dimensional graph where the x-axis is the input feature and the y-axis is the output feature for each network. This graph is hence a fully transparent function describing how the neural network learned to transform the input feature as it contributes, additively, to the prediction. Hence this type of neural architecture is sufficient for creating a model as transparent as the linear models described in this blog postso far. . Below is code for creating a GANN using Keras. As you can see, this model is capable of solving the regression problem while maintaining feature-wise transparency on piecewise continous functions! . from tensorflow.keras import layers, regularizers #define a simple multi-layer perceptron class MLP(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) # relu helps learn more jagged functions if necessary. self.l1 = layers.Dense(8, activation=&#39;relu&#39;) # softplus helps smooth between the jagged areas from above # as softplus is referred to as &quot;SmoothRELU&quot; self.l2 = layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Generalized Additive Neural Network for n features class GANN(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features # initialize MLP for each input feature self.components = [MLP() for i in range(n_features)] # create final layer for a linear combination of learned components self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): #split up by individual features individual_features = tf.split(x, self.n_features, axis=1) components = [] #apply the proper MLP to each individual feature for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) #concatenate learned components and return linear combination of them components = tf.concat(components, axis=1) return self.linear_combination(components) . model = GANN(n_features=3) model.compile( optimizer=tf.keras.optimizers.Adam(), loss=&#39;mean_squared_error&#39;, metrics=[&#39;mean_absolute_error&#39;] ) hist = model.fit( X.to_numpy(), y.to_numpy(), epochs=50, batch_size=32, ) . At first glance, these results look good, but not perfect. While the Neural Additive Model was capable of learning the general shapes of the functions, it looks like it&#39;s off on the intercepts for all of them. This should not discount the validity of the model. The corresponding math demonstrates perfectly fitting intercepts of an additive model cannot be guaranteed. . Let $h_i(X_i) = alpha_i + f(X_i)$ where $f(X_i)$ represents all of the aspects of $h(X_i)$ that dependent on $X_i$, and $ alpha_i$ represents the intercept. . $$ begin{aligned} y &amp; = beta_0 + sum_{i=1}^n beta_i h_i(X_i) &amp; = beta_0 + sum_{i=1}^n beta_i ( alpha_i + f_i(X_i)) &amp; = beta_0 + sum_{i=1}^n beta_i alpha_i + sum_{i=1}^n beta_i f_i(X_i) end{aligned} $$The only way to tease apart these intercepts is via $ beta$. Imagine the proper fit of this equation, for every $i$, had $ beta_i = 1$ and $ alpha_i = 2$. In this case, if half of the learned $ alpha_i$s are zero, and the other half are four, that would yield the exact same result for $ sum_{i=1}^n beta_i alpha_i$ as the proper fit. Hence, by way of contradictory example, it is impossible to guarantee learning correct intercepts for the individual components of any additive model. . Luckily, the &quot;goodness of fit&quot; of this model to the true functions can be shown by comparing the partial derivatives. This is because, when taking a derivative with respect to $X$, any aspects of the equation that don&#39;t depend on $X$ (e.g. the intercepts) become zero. . This is a clear deomonstration that Generalized Additive Neural Networks are capable of overfitting to piecewise continuous functions while maintaining transparency! . However, this is still just a toy example. The next section delves into Neural Additive Models (NAMs), which are a special variant of GANNs that are designed to be able to fit &quot;jumpy functions&quot;, as this is more reminiscient of real-world data. . Neural Additive Models (NAMs) . (Agarwal et al., 2020) . Generalizing the Computational Graph . This is the fun part. This is where all your creativity can yield an extremely flexible, powerful, and interpretable model! The world is your oyster, and you can use your understanding of neural networks and additive models to design an architecture that accomplishes your goals. . References Hastie, T., &amp; Tibshirani, R. (1986). Generalized Additive Models. Statist. Sci., 1(3), 297–310. https://doi.org/10.1214/ss/1177013604 | Hornik, K., Stinchcombe, M., &amp; White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5), 359–366. https://doi.org/https://doi.org/10.1016/0893-6080(89)90020-8 | Pinkus, A. (1999). Approximation theory of the MLP model in neural networks. Acta Numerica, 8, 143–195. https://doi.org/10.1017/S0962492900002919 | Balázs, C. (2001). Approximation with Artificial Neural Networks. https://doi.org/10.1.1.101.2647 | Potts, W. J. E. (1999). Generalized Additive Neural Networks. Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 194–200. https://doi.org/10.1145/312129.312228 | Agarwal, R., Frosst, N., Zhang, X., Caruana, R., &amp; Hinton, G. E. (2020). Neural Additive Models: Interpretable Machine Learning with Neural Nets. | . Footnotes . The closed form solution for OLS is $ beta = (X^TX)^ inv X^Ty$. This requires $X^TX$ to be invertible, which is the case when the elements in $X$ are linearly independent. This is satisfied by our assumption of independence. Without this assumption, there is no closed form solution, and $ beta$ can be approximated by the maximum likelihood estimation function: $min_ beta(y - beta X)^T(y - beta X)$.↩ | The exponential family is a particular family of probability distributions such that their probability density function (PDF) can be writted as: $P(x | theta) = f(x) g( theta) exp Big( eta( theta) centerdot T(x) Big)$, where $f$, $g$, $ eta$, and $T$ are known functions and $ theta in Reals$ is the only parameter to the PDF.↩ | This explains why we could create a model identical to linear and logistic regression by designing a neural network with zero hidden layers.↩ | The &quot;smoothness&quot; of a function is described by the continuity of the derivatives. The set of functions with a smoothness of 0 is equivalent to the set of continuous functions. The set of functions with a smoothness of 1 is the set of continuous functions such that their first derivative is continuous. So on, and so forth. Generally, a function is considered &quot;smooth&quot; if it has &quot;smoothness&quot; of $ infty$. In other words, it is infinitely differentiable.↩ | Technically, this could be fit where the sub-networks take more than a single feature as input, but this comes at a cost of interpretability. It is still possible to explore the relationship between both features and the output, however it becomes high-dimensional, entangled, and hence more difficult to interpret.↩ |",
            "url": "https://ryansaxe.com/transparency/2020/12/01/NAM.html",
            "relUrl": "/transparency/2020/12/01/NAM.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  

  

  

  

  
  

  

  
  

  

  

  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}