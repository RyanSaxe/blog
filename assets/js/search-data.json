{
  
    
        "post0": {
            "title": "Designing Interpretable Neural Networks",
            "content": "Any supervised prediction problem can be described as learning some function $f$ that minimizes the error to the equation $f(x) = y$, where $x$ is the input features and $y$ is the target prediction. This blog post will explore methods to devise neural architectures to learn such functions, with the additional goal of making them as transparent as linear models. . Prior to delving straight into the neural networks, we should work our way up the hierarchy of linear models. Starting from basic linear models, to generalizing them with the aptly named General Linear Model (GLM), and eventually removing linearity with Generalized Additive Models (GLMs) and beyond! . Linear Regression . Different algorithms are sufficient for learning different families of functions. For example, simple linear models like linear regression can only learn to make predictions according to functions of the form: . $$y = X^T beta + epsilon = beta_0 + beta_1 X_1 + beta_2 X_2 + cdots + beta_N X_N + epsilon$$ . Where $ beta_i$ represents learned coefficients with respect to $X_i$, and $ epsilon$ is the error-term (often referred to as the residual). Furthermore, linear regression comes with some additional assumptions: . Linearity: The relationship between $X$ and the mean of $Y$ . The namesake of linear models: linearity. This is a rigid assumption. Linear regression cannot properly solve a problem where this does not hold. In fact, a lack of this relationship will often be reflected in the error terms in a way that breaks the other assumptions. . | Independence: Observations are independent of each other. . Linear regression assumes that there are no relationships between the features. For example, a model that has a feature for price and a feature for discount would break this assumption. As discount changes, the price changes accordingly, creating a dependence between these features. This type of dependence is referred to as multicolinearity. . | Normality: $y$ given any $X$ comes from a normal distribution. . More specifically, we say that $y in mathcal{N}(X^T beta, sigma^2 I)$, which also implies that the distribution of the residuals is normal and independent of $X$. Furthermore, this means we can consider linear regression as predicting the mean of a normal distribution. . | $$y = mathbb{E}[y|X] = mu(x) = X^T beta + epsilon$$ . Homoscedasticity: The variance of residual is the same for any value of $X$ . This is a direct corollary from $y in mathcal{N}(X^T beta, sigma^2 I)$. $ sigma^2 I$ implies that the variance of the distribution is not dependent on $X$, and each $X$ has the same variance. Heteroscedasticity often implies a lack of linearity, independence, or normality. It&#39;s a hint that there are complexities in the data that cannot be represented by your current model. If your linear model has variance within the error terms, this implies that you should probably explore other methodologies, as a classic linear model was not able to properly capture $ mathbb{E}[y|X]$. . | These assumptions are quite rigid for the real world. Many datasets and/or problem spaces do not conform to these restrictions. So why do we still use linear regression when we have algorithms that can comparably perform the regression task without such rigid assumptions? Two common answers to this question are: . Occam&#39;s Razor: Don&#39;t add complexity without necessity. . Linear regression is the simplest way of solving a regression problem, and it&#39;s often a good idea to start simple. . | Interpretability: The decisions of the model can be explained with respect to $X$ and $y$. . Linear regression fits an exact equation known apriori. When the model spits out $ beta_2$, that means that, according to the model, the feature $X_2$ contributes $ beta_2 X_2$ to the outcome. . | In many scenarios, the learned $ beta$ coefficients are more valuable than the actual prediction. Let&#39;s say you (unadvisably so) devised a linear model for predicting the expected salary of an individual, and that model (also unadvisably so) contained sensitive factors like race or gender. If the model learns high coefficients with respect to those features, well, you have a sexist and/or racist model! While you probably shouldn&#39;t have even attempted building this model in the first place, at least it was fully transparent so you know it is unfair and hence won&#39;t deploy it (I hope). . Today, we are going to stick with the notion that transparency is of the utmost importance. Whatever the model is, it must be able to produce feature-wise explanations that are useful. However, contrary to what you may have heard, these models don&#39;t need to be exactly linear as described above. . Generalized Linear Models . Generalized Linear Models (GLMs) are exactly what they sound like, a generalization of the definitions described above. Technically, linear regression is a GLM. However, the set of potential GLMs describe a much wider array of functions. . GLMs loosen the constraints of normality. Now the mean of the distribution connecting $y$ to $X$ can represent an expectation from any distribution in the exponential family1. This includes quite a lot of extra distributions, such as normal, bernoulli, poisson, and more! . However, not all of these distributions range from $- infty$ to $+ infty$, which is necessary for solving regression problems in the linear form $y = X^T beta$. In order for us to maintain a linear model, we transform the mean of the distribution ( $ mu(x) = mathbb{E}[y|X]$ ) using a specified function $g$. $g$ is called the link function. . $$g( mu(x)) = X^T beta + epsilon$$ . Observe that this also loosens the constraints of linearity. Recall that $ mu(x)$ is just our expectation for $y$ given $X$. So, if we substitute back $y$, and take the inverse to get rid of $g$ in relationship to $y$, we get: . $$y = g^{-1}(X^T beta) + epsilon$$ . Hence the relationship between $y$ and $X$ is no longer required to be exactly linear. However, the relationship between the resulting $g(y)$ transformed by the link function and $X$ must be linear. . Note that this clarification implies that L $g$ is the identity function $g( mu) = mu$, then the GLM is just normal linear regression. . Logistic Regression . Logistic regression is actually a GLM! For simplicity, this section will cover only binary logistic regression. This means that $y$ will be either 0 or 1, and hence can be described by a bernoulli distribution. In order to squash the regression results to be between 0 and 1, we use the logit link function $g( mu) = log( frac{ mu}{1 - mu})$. . Relating to Neural Networks . class LogisticRegression(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) self.output_layer = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) def call(self, x, training=None): return self.output_layer(x) . Train on 100000 samples Epoch 1/5 100000/100000 [==============================] - 4s 37us/sample - loss: 0.1670 - mean_absolute_error: 0.3911 Epoch 2/5 100000/100000 [==============================] - 3s 33us/sample - loss: 0.0960 - mean_absolute_error: 0.2786 Epoch 3/5 100000/100000 [==============================] - 3s 33us/sample - loss: 0.0733 - mean_absolute_error: 0.2272 Epoch 4/5 100000/100000 [==============================] - 3s 33us/sample - loss: 0.0618 - mean_absolute_error: 0.1974 Epoch 5/5 100000/100000 [==============================] - 3s 34us/sample - loss: 0.0545 - mean_absolute_error: 0.1775 . Generalized Additive Models . GAMs take another step towards reducing the restrictions within linear models. There is exactly one modification on GLMs that turn them into GAMs: . $$y = beta_0 + beta_1 h_1(X_1) + beta_2 h_2(X_2) + cdots + beta_N h_N(X_N) + epsilon$$ . where each function $h_i$ is allowed to be a non-linear function. This eliminates the restriction of linearity. Furthermore, GAMs are allowed to model dependent features . gam = LinearGAM(spline(0) + spline(1) + spline(2)).fit(X, y) . Neural Additive Models . # relu helps learn more jagged functions if necessary class MLP(tf.keras.Model): def __init__( self, name=None ): super().__init__(name=name) self.l1 = tf.keras.layers.Dense(8, activation=&#39;relu&#39;) self.l2 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.l3 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.l4 = tf.keras.layers.Dense(8, activation=&#39;softplus&#39;) self.output_layer = tf.keras.layers.Dense(1) def call(self, x, training=None): x = self.l1(x) x = self.l2(x) x = self.l3(x) x = self.l4(x) return self.output_layer(x) #define a Neural Additive Model for n features class NAM(tf.keras.Model): def __init__( self, n_features, name=None ): super().__init__(name=name) self.n_features = n_features self.components = [MLP() for i in range(n_features)] self.linear_combination = tf.keras.layers.Dense(1) @tf.function def call(self, x, training=None): individual_features = tf.split(x, self.n_features, axis=1) components = [] for f_idx,individual_feature in enumerate(individual_features): component = self.components[f_idx](individual_feature) components.append(component) components = tf.concat(components, axis=1) return self.linear_combination(components) . Train on 1000000 samples Epoch 1/10 1000000/1000000 [==============================] - 60s 60us/sample - loss: 414.4350 - mean_absolute_error: 10.9988 Epoch 2/10 1000000/1000000 [==============================] - 57s 57us/sample - loss: 120.1113 - mean_absolute_error: 4.9874 Epoch 3/10 1000000/1000000 [==============================] - 55s 55us/sample - loss: 74.6649 - mean_absolute_error: 3.6563 Epoch 4/10 1000000/1000000 [==============================] - 54s 54us/sample - loss: 59.5442 - mean_absolute_error: 3.0831 Epoch 5/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 51.1148 - mean_absolute_error: 2.6067 Epoch 6/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 46.4825 - mean_absolute_error: 2.3151 Epoch 7/10 1000000/1000000 [==============================] - 55s 55us/sample - loss: 43.2321 - mean_absolute_error: 2.1660 Epoch 8/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 41.0939 - mean_absolute_error: 2.0531 Epoch 9/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 39.4331 - mean_absolute_error: 1.9772 Epoch 10/10 1000000/1000000 [==============================] - 56s 56us/sample - loss: 37.6433 - mean_absolute_error: 1.9064 . &lt;tensorflow.python.keras.callbacks.History at 0x21904d6d948&gt; . 1. which just means that an exponential shows up in the probility density function (pdf)↩ .",
            "url": "https://ryansaxe.com/transparency/2020/12/01/NAM.html",
            "relUrl": "/transparency/2020/12/01/NAM.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  

  

  

  
  

  

  
  

  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ryansaxe.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}